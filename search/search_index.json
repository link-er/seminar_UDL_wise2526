{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Seminar \"Understanding Deep Learning\" (WS25/26)","text":"<p>This seminar is based on the open-access book Understanding Deep Learning by Simon Prince.</p> <p>Each week, one or more students will present a chapter or topic and document the results here. The goal is to create a collaborative, semester-long learning resource, including presentation summaries, discussion notes, and reports.</p>"},{"location":"#announcements","title":"Announcements","text":"<ul> <li>On 3rd November I have to attend my research group meeting to give a presentation - I would ask you to start around 1.5 hours later, so at 15.30. Please let me know if it does not work for you.</li> <li>Presentations will be stored (at least for the time of the seminar) here</li> </ul>"},{"location":"#semester-details","title":"Semester Details","text":"<ul> <li>Dates: 20 Oct 2025 \u2013 2 Feb 2026  </li> <li>Venue: room OH12/1.054</li> <li>Meetings: Mondays, weekly, 14.00-16.00</li> <li>Winter break: 22 Dec 2025 \u2013 2 Jan 2026  </li> </ul>"},{"location":"#student-responsibilities","title":"Student Responsibilities","text":"<ul> <li>Present once: Each student is responsible for presenting one chapter or part of combined topic.  </li> <li>Document presentations: After each session, prepare a written report summarizing key points of the chapter or combined topic.  </li> <li>Discussion: Each student (or group of two) is responsible for discussion for one topic, different from the presented one. After each session, prepare a written report summarizing key discussion outcomes.  </li> <li>Contribute to the GitHub site: Upload your report in the appropriate folder on the repository.</li> </ul>"},{"location":"#resources","title":"Resources","text":"<ul> <li>Report Template: </li> </ul> <pre><code>\\# Report \u2013 Week XX: \\[Chapter Title]\n\n\n\n\\*\\*Presenter:\\*\\* \\[Name]  \n\n\\*\\*Date:\\*\\* \\[DD.MM.YYYY]  \n\n\n\n\\## Summary\n\n(1\u20132 page summary of key concepts, equations, and methods)\n\n\n\\## Discussion Notes\n\n\\- Key questions raised during seminar\n\n\\- Open problems or unclear points\n\n\n\n\\## References\n\n\\- Links to relevant papers, blog posts, code\n</code></pre> <ul> <li>Book Chapters: Access online here</li> </ul> <p>The detailed seminar schedule is available on the Schedule Page.</p>"},{"location":"schedule/","title":"Seminar Schedule: Understanding Deep Learning (WS 25/26)","text":"<ul> <li>Meetings: Mondays, weekly (20 Oct 2025 \u2013 6 Feb 2026), 14.00-16.00, room OH12/1.054</li> <li>Winter break: 22 Dec 2025 \u2013 2 Jan 2026</li> <li>Total students: 19</li> </ul> Date Chapters Pages (approx.) Students Notes 20 Oct 2025 Ch. 1\u20132: Introduction + Supervised Learning \u2013 Instructor Intro lecture, organization, topic assignment 27 Oct 2025 Ch. 3\u20134: Shallow &amp; Deep Neural Networks 22 P gr1, D gr2 Fundamentals of network architectures 3 Nov 2025 Ch. 5\u20136: Loss Functions &amp; Fitting Models 30 2 Training objectives and optimization setup 10 Nov 2025 Ch. 7: Gradients &amp; Initialization 15 1 Backpropagation &amp; parameter initialization 17 Nov 2025 Ch. 8\u20139: Performance &amp; Regularization 31 2 Evaluation metrics and regularization techniques 24 Nov 2025 Ch. 10\u201311: CNNs &amp; Residual Networks 31 2 Convolutional structures and ResNets 1 Dec 2025 Ch. 12: Transformers 25 2 Attention and transformer architectures 8 Dec 2025 Ch. 13: Graph Neural Networks 21 1 Graph-based representations 15 Dec 2025 Ch. 14\u201315: Unsupervised Learning &amp; GANs 27 2 Representation learning and generative adversarial models 22 Dec \u2013 2 Jan Winter break \u2013 \u2013 No meeting 5 Jan 2026 Ch. 16: Normalizing Flows 17 1 Flow-based generative modeling 12 Jan 2026 Ch. 17: Variational Autoencoders 16 1 Probabilistic latent variable models 19 Jan 2026 Ch. 18: Diffusion Models 19 1 Modern generative diffusion approaches 26 Jan 2026 Ch. 19: Reinforcement Learning 22 2 Policy gradients and deep RL 2 Feb 2026 Ch. 20\u201321: Why Deep Learning Works &amp; Ethics 31 1 Conceptual and societal discussion <p>Groups: - gr1: Ponikarov Antonia - gr2: Zoghlami Fadi, Ben Halima Ibrahim</p>"},{"location":"reports/week01_kickoff/","title":"Report \u2013 Week 01: Shallow &amp; Deep Neural Networks","text":"<p>Presenter: [Name]  </p> <p>Date: 27.10.2025  </p>"},{"location":"reports/week01_kickoff/#summary","title":"Summary","text":"<p>(1\u20132 page summary of key concepts, equations, and methods)</p>"},{"location":"reports/week01_kickoff/#discussion-notes","title":"Discussion Notes","text":"<ul> <li> <p>Why is ReLU the most used activation function in practice ?</p> <p>ReLU is the most widely used activation function in practice because it offers a simple yet powerful balance between computational efficiency and effective gradient propagation.</p> <p>Other activation functions like sigmoid or tanh, for example, squash inputs into narrow ranges which makes their gradients very small for large input magnitudes ==&gt; Vanishing gradients problem [Link]</p> <p>ReLU on the other hand allows gradients to flow efficiently through the different layers of the network. Its derivative of the output with respect to the input is always a constant 1  for positive inputs. This helps to avoid the saturation problem (derivative becomes close to zero) for large input data that the derivatives of the sigmoid activation function suffer from.</p> <p>Another advantage of using ReLU is that it is extremely simple to compute (we just compare with zero and clip negative inputs) and has no exponentials or divisions like sigmoid or tanh.</p> </li> <li> <p>Bias-Variance tradeoff :</p> <p>When training our Neural Network/Model, our goal is to predict well on new unseen data. The Bias-Variance tradeoff describes how the model complexity affects prediction error, and why we need to balance Underfitting and Overfitting.</p> <p>Bias :    + Measures how far the model\u2019s predictions are from the true function on average    + It represents systematic error (how much the model \u201cmisses the mark\u201d)    + High bias ==&gt; Underfitting (model too simple, can\u2019t capture patterns)</p> <p>Variance :    + Measures how much the model\u2019s predictions vary if we train it on different datasets drawn from the same distribution    + High variance means the model memorizes noise instead of learning the underlying patterns    + High variance ==&gt; Overfitting (Model does not generalize well)</p> <p>The Tradeoff :    + Simple Network (Low complexity): if Bias High and Variance Low ==&gt; Underfitting    + Complex Network (High complexity): if Bias Low and Variance High ==&gt; Overfitting</p> <p></p> </li> <li> <p>What are the advantages of using Deep Neural Networks over Shallow ones ?</p> <p>Deep Neural Networks are generally preferred over Shallow Neural Networks because they can model complex patterns in data more effectively thanks to their depth. While shallow networks can theoretically approximate any function, they would need an exponentially larger number of neurons to achieve the same level of performance ==&gt; Higher representation power</p> <p>Deep Neural Networks tend also to generalize better on new unseen data. Shallow networks, on the other hand, often underfit and fail to capture the underlying structure of complex datasets ==&gt; Better generalization</p> <p>Deep networks use their layers to build on previously learned features, allowing for the reuse of information. This means they can achieve strong performance without having to drastically increase the number of parameters, which makes them more efficient than wide shallow networks ==&gt; Parameter efficiency</p> </li> <li> <p>What does folding the input space actually mean ? (TODO)</p> </li> </ul>"},{"location":"reports/week01_kickoff/#references","title":"References","text":"<ul> <li> <p>Deep Learning using Rectified Linear Units (ReLU) [Link]</p> </li> <li> <p>A Modern Take on the Bias-Variance Tradeoff in Neural Networks [Link]</p> </li> </ul>"}]}