{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Seminar \"Understanding Deep Learning\" (WS25/26)","text":"<p>This seminar is based on the open-access book Understanding Deep Learning by Simon Prince.</p> <p>Each week, one or more students will present a chapter or topic and document the results here. The goal is to create a collaborative, semester-long learning resource, including presentation summaries, discussion notes, and reports.</p>"},{"location":"#announcements","title":"Announcements","text":"<ul> <li>On 3rd November I have to attend my research group meeting to give a presentation - I would ask you to start around 1.5 hours later, so at 15.30. Please let me know if it does not work for you.</li> <li>Presentations will be stored (at least for the time of the seminar) here</li> <li>On 1st December my collegue Tim Katzke will replace me while I am away at a conference.</li> </ul>"},{"location":"#semester-details","title":"Semester Details","text":"<ul> <li>Dates: 20 Oct 2025 \u2013 2 Feb 2026  </li> <li>Venue: room OH12/1.054</li> <li>Meetings: Mondays, weekly, 14.00-16.00</li> <li>Winter break: 22 Dec 2025 \u2013 2 Jan 2026  </li> </ul>"},{"location":"#student-responsibilities","title":"Student Responsibilities","text":"<ul> <li>Present once: Each student is responsible for presenting one chapter or part of combined topic.  </li> <li>Document presentations: After each session, prepare a written report summarizing key points of the chapter or combined topic.  </li> <li>Discussion: Each student (or group of two) is responsible for discussion for one topic, different from the presented one. After each session, prepare a written report summarizing key discussion outcomes.  </li> <li>Contribute to the GitHub site: Upload your report in the appropriate folder on the repository.</li> </ul>"},{"location":"#resources","title":"Resources","text":"<ul> <li>Report Template: </li> </ul> <pre><code>\\# Report \u2013 Week XX: \\[Chapter Title]\n\n\n\n\\*\\*Presenter:\\*\\* \\[Name]  \n\n\\*\\*Date:\\*\\* \\[DD.MM.YYYY]  \n\n\n\n\\## Summary\n\n(1\u20132 page summary of key concepts, equations, and methods)\n\n\n\\## Discussion Notes\n\n\\- Key questions raised during seminar\n\n\\- Open problems or unclear points\n\n\n\n\\## References\n\n\\- Links to relevant papers, blog posts, code\n</code></pre> <ul> <li>Book Chapters: Access online here</li> </ul> <p>The detailed seminar schedule is available on the Schedule Page.</p>"},{"location":"schedule/","title":"Seminar Schedule: Understanding Deep Learning (WS 25/26)","text":"<ul> <li>Meetings: Mondays, weekly (20 Oct 2025 \u2013 6 Feb 2026), 14.00-16.00, room OH12/1.054</li> <li>Winter break: 22 Dec 2025 \u2013 2 Jan 2026</li> <li>Total students: 19 11</li> </ul> Date Chapters Pages (approx.) Students Notes 20 Oct 2025 Ch. 1\u20132: Introduction + Supervised Learning \u2013 Instructor Intro lecture, organization, topic assignment 27 Oct 2025 Ch. 3\u20134: Shallow &amp; Deep Neural Networks 22 P gr1, D gr2 Fundamentals of network architectures 3 Nov 2025 Ch. 5\u20136: Loss Functions &amp; Fitting Models 30 P gr3, D gr1 Training objectives and optimization setup 1710 Nov 2025 Ch. 8\u20139: Performance &amp; Regularization 31 P gr2, D gr3 Evaluation metrics and regularization techniques 1017 Nov 2025 Ch. 7: Gradients &amp; Initialization 15 P gr6, D gr7 Backpropagation &amp; parameter initialization 24 Nov 2025 - - - - 1 Dec 2025 Ch. 12: Transformers 25 P gr5, D gr6 Attention and transformer architectures 8 Dec 2025 Ch. 13: Graph Neural Networks 21 P gr8, D gr9 Graph-based representations 15 Dec 2025 Ch. 10\u201311: CNNs &amp; Residual Networks 31 P gr9, D gr5 Convolutional structures and ResNets 22 Dec \u2013 2 Jan Winter break \u2013 \u2013 No meeting 5 Jan 2026 Ch. 18: Diffusion Models 19 P gr7, D gr4 Modern generative diffusion approaches 12 Jan 2026 Ch. 19: Reinforcement Learning 22 P gr4, D gr8 Policy gradients and deep RL <p>Groups:</p> <ul> <li>gr1 Ponikarov Antonia </li> <li>gr2 Zoghlami Fadi, Ben Halima Ibrahim </li> <li>gr3 Filipiak Lucas, Romanovych Ivanna </li> <li>gr4 Marc Gl\u00e4ser </li> <li>gr5 Fadi Dalbah </li> <li>gr6 G\u00f6vert Noah </li> <li>gr7 Semler Alexander </li> <li>gr8 Vu Minh Nhat </li> <li>gr9 Pellecer Aisaiah </li> </ul>"},{"location":"reports/week01_kickoff/","title":"Report \u2013 Week 01: Introduction","text":"<p>Presenter: Linara Adilova</p> <p>Date: 20.10.2025  </p>"},{"location":"reports/week01_kickoff/#summary","title":"Summary","text":"<p>(1\u20132 page summary of key concepts, equations, and methods)</p>"},{"location":"reports/week02_ch3_4/","title":"Report \u2013 Week 02: Shallow &amp; Deep Neural Networks","text":"<p>Presenter: Antonia Ponikarov  </p> <p>Date: 27.10.2025  </p>"},{"location":"reports/week02_ch3_4/#summary","title":"Summary","text":"<p>Linear models are limited to representing simple, linear relationships between input $x$ and output $y$.  Consequently, certain problems cannot be solved using linear models, e.g. the XOR problem, which takes two binary  inputs and produces a binary output.  The output of the operation equals $1$ if and only if the two inputs differ.  The corresponding data points are not linearly separable, meaning that there is no way to draw a single line to  separate the two output classes. That example illustrates the necessity of neural networks, which introduce nonlinearity through activation functions. By composing multiple linear transformations with nonlinear activations, such models can represent flexible,  piecewise-linear functions and are therefore capable of modeling more complex mappings relationships.</p> <p>A shallow neural network can be defined as a function $\\textbf{y} = \\textbf{f} [\\textbf{x}, \\mathbf{\\phi]}$, parameterized by $\\mathbf{\\phi}$, which maps multivariate inputs $\\textbf{x}$ to multivariate outputs $\\textbf{y}$. </p> <p>Basic structure: \\ Consider the example of a network with a scalar input $x$ and a scalar output $y$, parameterized by ten parameters  $\\mathbf{\\phi} = {\\phi_0, \\phi_1, \\phi_2, \\phi_3, \\theta_{10}, \\theta_{11}, \\theta_{20}, \\theta_{21}, \\theta_{30}, \\theta_{31}}.$</p> <p>The corresponding input/output mapping is given by </p> <p>$$y = f[x,\\mathbf{\\phi}] = \\phi_0 + \\phi_1 \\text{a} [\\theta_{10} + \\theta_{11} x] + \\phi_2 \\text{a} [\\theta_{20} + \\theta_{21} x]+ \\phi_3 \\text{a} [\\theta_{30} + \\theta_{31} x].$$</p> <p>Interpretation of Parameters: - $\\theta_{ij}$: parameters defining the linear transformations (intercept and slope) applied to the input - $\\text{a}[\\cdot]$: nonlinear activation function - $\\phi_1, \\phi_2, \\phi_3$: output weights used to combine the activations - $\\phi_0$: offset/bias, which controls the height of the final function</p> <p>As illustrated in the figure above, a shallow network consists of a single hidden layer, which in this example contains  three hidden units, also referred to as neurons.  Each hidden unit computes a small linear function of the input, such as $\\theta_{10} + \\theta_{11} x$. Those intermediate values are known as pre-activations. An activation function is then applied to each pre-activation, for example $h_1=\\text{a}[\\theta_{10} + \\theta_{11} x]$.  Finally, the network output is obtained by linearly combining all activations using the output weights and adding a bias.</p> <p>The positions at which the hidden-unit contributions cross zero become the \"joints\" in the final output function.  At these locations, the slope of the output changes, resulting in a piecewise-linear structure. Each hidden unit contributes at most one such joint.  Consequently, a shallow network with three hidden units can represent a function consisting up to four linear regions.</p> <p>In the case of multivariate inputs and outputs, each hidden unit receives a weighted combination of all input  dimensions rather than a single scalar input. Similary, each output $y_j$ is formed as a weighted combination of all activations:</p> <p>$$\\displaystyle y_j = \\phi_{j0} + \\sum_{d = 1}^{D} \\phi_{jd}h_d$$ </p> <p>where the hidden-unit activations are defined as:</p> <p>$$\\displaystyle h_d = \\text{a}\\left[\\theta_{d0}+\\sum_{i=1}^{D_i}{\\theta_{di}x_i}\\right],$$</p> <p>with $\\mathbf{x} \\in \\mathbb{R}^{D_i}$ denoting the input vector and $\\textbf{y} \\in \\mathbb{R}^{D_j}$ the output vector.</p> <p>Activation Functions: \\ Neural Networks combine linear operations, such as weighted sums, with nonlinear activations.  Without nonlinear activations, the composition of multiple layers would collapse to a single linear mapping, regardless  of network depth.  Activation functions therefore play a crucial role by introducing nonlinearity at each hidden unit, enabling neural  networks to model nonlinear input/output mappings.</p> <p>The most commonly used activation function is the Rectrified Linear Unit (ReLU).  It keeps positive values unchanged while mapping negative values to zero, thereby inducing a piecewise-linear behavior.  ReLU activations enable efficient optimization and often lead to sparse activations. However, ReLU also suffers from the so-called \"dying ReLU problem\":  if a neuron's pre-activation remains negative, its outputs and gradient become zero, preventing further updates of the  corresponding weights during training.</p> <p>This issue can be resolved by using variants such as the Leaky ReLU function, which assigns a small non-zero slope to  negative input values and thus preserves gradient flow.  Other activation functions, including the sigmoid, tanh, and Exponential Linear Unit (ELU), have also been proposed to  address limitations of standard ReLU activations. </p> <p>Universal Approximation Theorem:\\ The Universal Approximation Theorem states that, for any continuous function, there exists a shallow network that can  approximate this function arbitrarily well, provided a sufficient number of hidden units is available.  When ReLU activation functions are used, each hidden unit adds at most one joint to the resulting function.  Consequently, a shallow network with $D$ hidden units can represent a piecewise-linear function consisting of at most  $D$ joints and therefore at most $D+1$ linear regions.  Increasing the number of hidden units increases the number of linear regions. As the number of linear regions grows, each region covers a small section of the input domain, enabling the network to  more closely match the shape of the true underlying continuous function.  However, this theorem proves existence, not efficiency:  while shallow networks are theoretically capable of approximating arbitrary continuous functions, they may need a large amount of hidden units to do so accurately for complex functions.</p> <p>Limitations of Shallow Networks: \\ Approximating complex functions, especially in high dimensional input spaces, typically requires an impractically large  number of hidden units.  As a result, the number of parameters grows rapidly with both the input dimension and hidden units, leading to poor  parameter inefficiency.  In contrast, depth enables far more regions with fewer parameters, allowing deep neural networks to achieve the same  expressivity.</p> <p>Deep Networks:\\ In contrast to shallow networks, which contain only a single hidden layer, deep neural networks consist of multiple  hidden layer stacked sequentially. </p> <p>Composing two single-layer networks, each with three hidden units (see figure above on the left-hand side), results in  the first network to create three alternating slopes, mapping the input domain into the interval $[-1, 1]$.  The second network then applies a similar transformation to this intermediate representation.  Through this repeated folding and reuse of input ranges, the composed network generates nine linear regions instead of  three, as illustrated in the figure above on the right-hand side.</p> <p>When two shallow networks are combined, it essentially yields a special case of a deep network with two hidden layers. </p> <p>The hidden units $h_1$, $h_2$ and $h_3$ in the first hidden layer are computed as usual by forming linear functions of  the input and applying the ReLU activation function on them.  Their outputs then serve as inputs to the second hidden layer, where the pre-activation values are computed by taking  three new linear functions of these hidden units.  At this point, there are three piecewise linear functions where the joints between linear regions are at the same  places (see figure a)-c) below). Applying a second ReLU activation to each function, introduces additional joints (see figure d)-f) below), further  increasing the number of linear regions. After weightening the functions and combining them together, the final output is a linear combination of these hidden  units:</p> <p>$$y' = \\phi'_0 + \\phi'_1h'_1 + \\phi'_2 h'_2 + \\phi'_3 h'_3.$$</p> <p>General Form - Matrix Notation:\\ A deep neural network with $K$ hidden layers can be compactly described as a sequence of linear transformations  alternating with nonlinear activation functions:</p> <p>$$ \\begin{aligned} \\mathbf{h}1 &amp;= \\mathbf{a}[\\mathbf{\\beta}_0 + \\mathbf{\\Omega}_0 \\mathbf{x}] \\\\ \\mathbf{h}_2 &amp;= \\mathbf{a}[\\mathbf{\\beta}_1 + \\mathbf{\\Omega}_1 \\mathbf{h}_1] \\\\ \\mathbf{h}_3 &amp;= \\mathbf{a}[\\mathbf{\\beta}_2 + \\mathbf{\\Omega}_2 \\mathbf{h}_2] \\\\ &amp;\\vdots \\\\ \\mathbf{h}_K &amp;= \\mathbf{a}[\\mathbf{\\beta}{K-1} + \\mathbf{\\Omega}{K-1} \\mathbf{h}{K-1}] \\\\ \\mathbf{y} &amp;= \\mathbf{\\beta}_K + \\mathbf{\\Omega}_K \\mathbf{h}_K \\end{aligned} $$</p> <ul> <li>$\\mathbf{h}_k$ : vector of hidden units at layer $k$</li> <li>$\\mathbf{\\beta}_k$ : vector of biases that contributes to layer $k+1$</li> <li>$\\mathbf{\\Omega}_k$ : weights that are applied to $k^{\\text{th}}$ layer and contributes to $(k + 1)^{\\text{th}}$ - layer</li> </ul> <p>Depth vs. Width:\\ To sum up, both shallow and deep networks can approximate any continuous function given enough capacity. However, depth provides a significantly more parameter-efficient means of achieving expressive power. A shallow network with one input, one output and $D &gt; 2$ hidden units can represent at most $D + 1$ linear regions  using $3D + 1$ parameters.  In contrast, a deep network with $K$ hidden layers of width $D &gt; 2$ can create up to $(D+1)^k$ linear regions with a  comparable parameter budget (exactly $3D + 1 + (K-1)D(D+1)$ parameters).  The exponential growth in the number of linear regions explains why deep networks are particularly well suited for modeling complex, structured data such as images.  In practice, deep networks achieve state-of-the-art performance across most tasks.</p>"},{"location":"reports/week02_ch3_4/#discussion-notes","title":"Discussion Notes","text":"<ul> <li> <p>Why is ReLU the most used activation function in practice ?</p> <p>ReLU is the most widely used activation function in practice because it offers a simple yet powerful balance between computational efficiency and effective gradient propagation.</p> <p>Other activation functions like sigmoid or tanh, for example, squash inputs into narrow ranges which makes their gradients very small for large input magnitudes ==&gt; Vanishing gradients problem (read more ...)</p> <p>ReLU on the other hand allows gradients to flow efficiently through the different layers of the network. Its derivative of the output with respect to the input is always a constant 1  for positive inputs. This helps to avoid the saturation problem (derivative becomes close to zero) for large input data that the derivatives of the sigmoid activation function suffer from.</p> <p>Another advantage of using ReLU is that it is extremely simple to compute (we just compare with zero and clip negative inputs) and has no exponentials or divisions like sigmoid or tanh.</p> </li> <li> <p>Bias-Variance tradeoff :</p> <p>When training our Neural Network/Model, our goal is to predict well on new unseen data. The Bias-Variance tradeoff describes how the model complexity affects prediction error, and why we need to balance Underfitting and Overfitting.</p> <p>Bias :  - Measures how far the model\u2019s predictions are from the true function on average  - It represents systematic error (how much the model \u201cmisses the mark\u201d)  - High bias ==&gt; Underfitting (model too simple, can\u2019t capture patterns)</p> <p>Variance :  - Measures how much the model\u2019s predictions vary if we train it on different datasets drawn from the same distribution  - High variance means the model memorizes noise instead of learning the underlying patterns  - High variance ==&gt; Overfitting (Model does not generalize well)</p> <p>The Tradeoff :  - Simple Network (Low complexity): if Bias High and Variance Low ==&gt; Underfitting  - Complex Network (High complexity): if Bias Low and Variance High ==&gt; Overfitting</p> <p></p> </li> <li> <p>What are the advantages of using Deep Neural Networks over Shallow ones ?</p> <p>Deep Neural Networks are generally preferred over Shallow Neural Networks because they can model complex patterns in data more effectively thanks to their depth. While shallow networks can theoretically approximate any function, they would need an exponentially larger number of neurons to achieve the same level of performance ==&gt; Higher representation power</p> <p>Deep Neural Networks tend also to generalize better on new unseen data. Shallow networks, on the other hand, often underfit and fail to capture the underlying structure of complex datasets ==&gt; Better generalization</p> <p>Deep networks use their layers to build on previously learned features, allowing for the reuse of information. This means they can achieve strong performance without having to drastically increase the number of parameters, which makes them more efficient than wide shallow networks ==&gt; Parameter efficiency</p> </li> <li> <p>What does folding the input space actually mean ?</p> </li> </ul> <p>Folding the input space can be seen as applying a nonlinear transformation that helps make complex patterns much simpler and separable; meaning, wraping, bending or stretching (...) the input space so that same label points cluster and lie together in a region that is easier to separate rather than these points being far apart or hard to   separate from other points in the original space.</p> <p></p>"},{"location":"reports/week02_ch3_4/#references","title":"References","text":"<ul> <li> <p>Deep Learning using Rectified Linear Units (ReLU) [Link]</p> </li> <li> <p>A Modern Take on the Bias-Variance Tradeoff in Neural Networks [Link]</p> </li> </ul>"},{"location":"reports/week03_ch5_6/","title":"Report \u2013 Week 03: Loss Functions &amp; Fitting Models","text":"<p>Presenter: Ivanna Romanovych and Lucas Filipiak</p> <p>Date: 03.11.2025</p>"},{"location":"reports/week03_ch5_6/#summary","title":"Summary","text":""},{"location":"reports/week03_ch5_6/#loss-functions-ivanna-romanovych","title":"Loss Functions (Ivanna Romanovych)","text":""},{"location":"reports/week03_ch5_6/#the-general-recipe","title":"The General Recipe","text":"<p>The process of adapting a model $f[x, \\phi]$ to compute a probability distribution involves three steps:</p> <ol> <li>Choose a Distribution: Select a parametric probability distribution $Pr(y|\\theta)$ defined over the output domain $y$.</li> <li>Predict Parameters: Configure the machine learning model to predict one or more of the distribution's parameters ($\\theta$), such that $\\theta = f[x, \\phi]$.</li> <li>Minimize Negative Log-Likelihood: Train the model by finding parameters $\\phi$ that minimize the negative log-likelihood over the training dataset.</li> </ol>"},{"location":"reports/week03_ch5_6/#the-maximum-likelihood-criterion","title":"The Maximum Likelihood Criterion","text":"<p>The objective is to maximize the likelihood of the parameters. This approach implicitly assumes the data are independent and identically distributed (I.I.D). Because the product of many probabilities can become incredibly small and difficult to represent with finite precision arithmetic, we equivalently maximize the logarithm of the likelihood.</p> <p>Therefore, the final loss function $L[\\phi]$ becomes the minimization of the negative log-likelihood:</p> <p>$$\\hat{\\phi} = \\underset{\\phi}{\\mathrm{argmin}} \\left[ - \\sum_{i=1}^{I} \\log[Pr(y_i | f[x_i, \\phi])] \\right]$$</p>"},{"location":"reports/week03_ch5_6/#applications-of-the-recipe","title":"Applications of the Recipe","text":""},{"location":"reports/week03_ch5_6/#univariate-regression","title":"Univariate Regression","text":"<p>Goal: Predict a single scalar output $y \\in \\mathbb{R}$.</p> <ul> <li>Distribution: Univariate Normal distribution defined by mean $\\mu$ and variance $\\sigma^2$.</li> <li>Model Prediction: The model usually predicts the mean, $\\mu = f[x, \\phi]$.</li> <li>Loss Derivation: Substituting the Normal distribution into the negative log-likelihood formula and performing algebraic manipulations yields the Least Squares Loss function.   $$L[\\phi] = \\sum_{i=1}^{I} (y_i - f[x_i, \\phi])^2$$   This demonstrates that least squares loss naturally follows the assumption that predictions are drawn from a normal distribution.</li> </ul> <p>Heteroscedastic Regression: If the uncertainty of the model varies as a function of the input data, the model is referred to as heteroscedastic. In this case, the model predicts both the mean $\\mu$ and the variance $\\sigma^2$ (modeled as $f_2[x, \\phi]^2$).</p>"},{"location":"reports/week03_ch5_6/#binary-classification","title":"Binary Classification","text":"<p>Goal: Assign data to one of two discrete classes, where $y \\in {0, 1}$.</p> <ul> <li>Distribution: Bernoulli distribution, which has a single parameter $\\lambda$ representing the probability that $y=1$.</li> <li>Model Prediction: The model predicts $\\lambda$. To ensure the prediction is a valid probability $\\lambda \\in [0, 1]$, the network output is passed through a sigmoid function:   $$\\text{sig}[z] = \\frac{1}{1 + \\exp[-z]}$$</li> <li>Loss Function: The resulting loss is the Binary Cross-Entropy loss:   $$L[\\phi] = \\sum_{i=1}^{I} -(1-y_i)\\log[1-\\text{sig}[f[x_i, \\phi]]] - y_i\\log[\\text{sig}[f[x_i, \\phi]]]$$</li> </ul>"},{"location":"reports/week03_ch5_6/#multiclass-classification","title":"Multiclass Classification","text":"<p>Goal: Assign data to one of $K &gt; 2$ classes.</p> <ul> <li>Distribution: Categorical distribution with $K$ parameters ($\\lambda_1, ..., \\lambda_K$) representing the probability of each category.</li> <li>Model Prediction: To ensure parameters are positive and sum to one, the network outputs are passed through the softmax function:   $$\\text{softmax}k[z] = \\frac{\\exp[z_k]}{\\sum{k'=1}^{K} \\exp[z_{k'}]}$$</li> <li>Loss Function: This results in the Multiclass Cross-Entropy loss:   $$L[\\phi] = -\\sum_{i=1}^{I} \\log[\\text{softmax}_{y_i}[f[x_i, \\phi]]]$$</li> </ul>"},{"location":"reports/week03_ch5_6/#multiple-outputs","title":"Multiple Outputs","text":"<p>When a model makes multiple predictions simultaneously, the predictions are usually treated as independent. This implies the probability is a product of univariate terms, meaning the loss function becomes a sum of the negative log probabilities for each output.</p>"},{"location":"reports/week03_ch5_6/#connection-to-kl-divergence","title":"Connection to KL Divergence","text":"<p>The cross-entropy loss can be understood through the lens of Kullback-Leibler (KL) divergence. This metric evaluates the distance between the empirical distribution of the observed data, $q(y)$, and the model distribution, $Pr(y|\\theta)$.</p> <p>Minimizing the KL divergence between the empirical distribution (represented as weighted sum of point masses at the data points) and the model distribution is mathematically equivalent to minimizing the negative log-likelihood.</p> <p>$$\\hat{\\phi} = \\underset{\\phi}{\\mathrm{argmin}} \\left[ -\\sum_{i=1}^{I} \\log[Pr(y_i | f[x_i, \\phi])] \\right]$$</p>"},{"location":"reports/week03_ch5_6/#fitting-models-lucas-filipiak","title":"Fitting Models (Lucas Filipiak)","text":"<p>Figures and equations can be found on the slides</p>"},{"location":"reports/week03_ch5_6/#model-fitting","title":"Model Fitting","text":"<ul> <li>Objective: minimize loss on the training set by adjusting the parameters</li> <li>Process: known as model fitting</li> </ul>"},{"location":"reports/week03_ch5_6/#gradient-descent","title":"Gradient Descent","text":"<ul> <li>Basic idea: change parameters in the direction opposite to the gradient of the loss function, going \"downhill\" in loss space</li> <li>Initialization: parameters start with heuristic values.</li> </ul>"},{"location":"reports/week03_ch5_6/#example-1d-linear-regression","title":"Example: 1D Linear Regression","text":"<ul> <li>Model: linear function, 2 parameters</li> <li>Loss function: least squares </li> <li>Properties:<ul> <li>Convex loss surface  </li> <li>Local = global minimum</li> <li>No saddle points</li> </ul> </li> </ul>"},{"location":"reports/week03_ch5_6/#example-gabor-model","title":"Example: Gabor Model","text":"<ul> <li>Nonlinear function involving sinusoidal and exponential terms, 2 parameters</li> <li>Loss: least squares</li> <li>Properties:<ul> <li>Non-convex loss surface</li> <li>Local minima may not be global  </li> <li>Saddle points possible</li> <li>Gradient descent may get \"stuck\" at local minima and saddle points</li> </ul> </li> </ul>"},{"location":"reports/week03_ch5_6/#stochastic-gradient-descent-sgd","title":"Stochastic Gradient Descent (SGD)","text":"<ul> <li>Runs gradient descent iteratively on subsets of the training set</li> <li> <p>Pass of all training examples: epoch</p> </li> <li> <p>Advantages:</p> <ul> <li>Adds noise =&gt; helps escape local minima  </li> <li>Uses mini-batches =&gt; computationally cheaper per iteration  </li> <li>Improves generalization</li> </ul> </li> </ul>"},{"location":"reports/week03_ch5_6/#momentum-based-methods","title":"Momentum-based methods","text":""},{"location":"reports/week03_ch5_6/#sgd-with-momentum","title":"SGD with momentum","text":"<ul> <li> <p>Incorporates previous updates into next step</p> </li> <li> <p>Advantages:</p> <ul> <li>Smoother trajectory</li> <li>Reduced oscillation at valleys</li> </ul> </li> </ul>"},{"location":"reports/week03_ch5_6/#nesterov-accelerated-momentum","title":"Nesterov accelerated momentum","text":"<ul> <li>Computes gradient at position predicted by momentum</li> <li>Modification of SGD with momentum, same advantages</li> </ul>"},{"location":"reports/week03_ch5_6/#adaptive-movement-estimation-adam","title":"Adaptive movement estimation (Adam)","text":"<ul> <li>Combines all previous approaches.</li> </ul>"},{"location":"reports/week03_ch5_6/#modifications","title":"Modifications:","text":"<ol> <li>Disconnect step distance from gradient magnitude by normalizing the gradient</li> <li>Apply momentum</li> <li>Accelerate beginning movement</li> <li>Can operate in batches for efficiency</li> <li> <p>Optionally, apply in batches</p> </li> <li> <p>Advantages:</p> <ul> <li>Smooth trajectory</li> <li>Can converge at minima</li> </ul> </li> </ol>"},{"location":"reports/week03_ch5_6/#discussion-notes","title":"Discussion Notes","text":"<ul> <li> <p>Why does the presence of noise contributes to better finding the solution?</p> <p>Noise in gradient-based optimization, e.g. through stochastic gradient descent, helps the optimizer escape local minima. Noise promotes exploration of the parameter space and often leads to solutions with better generalization.</p> </li> <li> <p>What is a full-batch gradient descent?</p> <p>Full-batch gradient descent computes the gradient of the loss function using the entire training dataset for each update. While this yields stable and deterministic updates, it is computationally expensive and lack stochasticity that can  improve optimization in non-convex problems. </p> </li> <li> <p>What is the role of the learning rate, and how does it influence the behavior of gradient descent?</p> <p>The learning rate controls the step size of parameter updates in the direction of the negative gradient. Small learning rates lead to slow but stable convergence, whereas large learning rates accelerate training but may cause instability or divergence.   </p> </li> <li> <p>What is the advantage of computing both parameters $\\mu$ and $\\sigma^2$?</p> <p>Predicting both $\\mu$ and $\\sigma^2$ allows the model to quantify uncertainty in its predictions.  This is particularly important in probabilistic and Bayesian settings.</p> </li> <li> <p>When should Cross Entropy Loss be used, and when is Least Squares Loss more appropriate?</p> <p>Least Squares Loss is best suited for regression tasks under a Gaussian noise assumption. Cross Entropy Loss is more appropriate for classification tasks, as it models probability distributions and directly optimizes class likelihoods. </p> </li> </ul>"},{"location":"reports/week04_ch8_9/","title":"Report \u2013 Week 04: Performance &amp; Regularization","text":"<p>Presenters: Ben Halima Ibrahim and Zoghlami Fadi </p> <p>Date: 10.11.2025 </p>"},{"location":"reports/week04_ch8_9/#summary","title":"Summary","text":""},{"location":"reports/week04_ch8_9/#chapter-8-measuring-performance","title":"Chapter 8: Measuring performance","text":"<p>I - Training a simple model:</p> <p>A neural network with sufficient capacity will mostly perform well on the training dataset. However, this does not mean that it will generalize well on the testing dataset (which is normally new and unseen data for the model). This causes a big problem especially for real-world scenarios, where the model's performance has to be as good as possible.  =&gt; Our goal is to train a model that generalizes well on new data</p> <p>The test errors have three distinct causes:</p> <ul> <li>the inherent uncertainty in the task</li> <li>the amount of training data</li> <li>the choice of model</li> </ul> <p>In the first section of this chapter, a simple model is trained on the MNIST-1D dataset, which is 1D analogue of the MNIST dataset: each data example is created by randomly transforming one of the templates and adding noise.</p> <p></p> <p>Our simple model/neural network consists of D_i = 40 inputs and D_o = 10 outputs representing the number of classes the dataset has (numbers form 0 to 9). The neural network has 2 hidden layers each with D = 100 hidden units. Multiclass cross-entropy is used as a loss function with the Softmax function to produce class probabilities. The model is then trained for 6000 steps (150 epochs) using SGD (Stochastic Gradient Descent) as a learning algorithm with a learning rate of 0.1 and a batch-size of 100. After the training process, we tested our trained model on 1000 extra examples from the dataset.</p> <p></p> <p>In figure (a), we can see that the training error decreases as the training proceeds (the training data is classified perfectly after around 4000 training steps). The testing error, however, decreases as well but to about 40% and does not drop below it. In figure (b), the training loss also decreases continuously towards zero as the training proceeds. The testing loss, on the other hand, decreases at first but suddenly starts going up after around 1500 training steps reaching higher values than before. =&gt; Our model is making, in this case, the same mistakes but with increasing confidence and this will decrease the probability of correct answers, and therefore increase the negative log-likelihood =&gt; Our model has then memorized the training data but does not generalize well on the testing data</p> <p>II - Sources of error:</p> <p>When a neural network fails to generalize well, there are mainly three sources of error:</p> <ul> <li>Noise: the data generation process itself includes the addition of noise to the input data. Therefore, there are multiple possible valid outputs for each input (figure (a) below). This may be caused due to a stochastic element in the data generation process (mislabeled data as an example). In some rare cases, the noise can be absent: for example, a network might approximate a function that is deterministic but requires significant computation to evaluate. =&gt; However, noise is usually a fundamental limitaion on the test performance</li> <li>Bias: this happens when the model is not flexible enough to fit the data perfectly. In figure (b) below for example, the three-region model (cyan line) cannot exactly fit the true function (black line), even with the best possible parameters (gray regions represent signed error). </li> <li>Variance: this occurs when there are limited training examples, and therefore there is no way to distinguish noise in the underlying data from systematic changes in the underlying function.This means that, for different training datasets, the result will be slightly different each time (figure (c) below). In practice, however, there can be an additional variance due to the stochastic learning algorithm, which does not necesseraliy converge to the same solution each time.</li> </ul> <p></p> <ul> <li>Mathematical formulation of test error:</li> </ul> <p></p> <p>III - Reducing error:</p> <p>The Noise component is insurmountable, which means there is nothing we can do to avoid it. It represents a fundamental limit on expected model performance. However, we can reduce the Variance and Bias terms.</p> <ul> <li>Reducing Variance: variance results from limited noisy training data. This actually means that we can reduce it by increasing the quantity of our training data. This approach averages out the inherent noise and ensures that the input space is well sampled. The figure below shows the effect of training with three different samples (6, 10 and 100 samples). The best-fitting model for each dataset is then plotted: as we can see, with only 6 samples, the fitted function is different each time and the variance term is therefore significant. When we increase the number of samples, the fitted models become very similar and the variance term reduces as a result.</li> </ul> <p></p> <p>=&gt; In general, adding more training data almost always improves test performance.</p> <ul> <li>Reducing Bias: in order to reduce the bias term, we can increase the capacity of our model/neural network (number of hidden units and/or layers) which makes it more flexible and able to describe the true underlying function. The figure below shows the effect of increasing the number of linear regions (3, 5 and 10 regions): by increasing the number of linear regions, the model becomes flexible enough to fit the true function closely. As a result, the bias term decreases (gray region in a-c). Unfortuantely, this causes the variance term to go up (gray region d-f). =&gt; Increasing the model capacity does not necesseraliy reduce the test error ==&gt; Bias-Variance trade-off</li> </ul> <p></p> <ul> <li>Bias-Variance trade-off: when a model is too simple (low capacity), it ignores useful information, and the error is composed mostly of that from bias ==&gt; Underfitting When a model is too complex, it memorizes non-general patterns, and the error is composed mostly of that from variance ==&gt; Overfitting In both cases the model does not generalize well on new unseen data. The ideal model aims to minimize both bias and variance. It lays in the sweet-spot in between (not too simple, nor too complex) =&gt; Achieving such a balance will yield the minimum error</li> </ul> <p></p> Bias Variance Result Underfitting High Low Poor training and test performances Optimal Moderate Moderate Best generalization Overfitting Low High Poor test performance <ul> <li>Double descent: in classical machine learning, increasing the model complexity follows a U-shaped curve: test error decreases as bias drops, but eventually increases as the model overfits to noise (high variance). However, modern deep networks often exhibit double descent:</li> </ul> <p>The First Descent: Error drops as the model learns the underlying function.</p> <p>The Interpolation Threshold: As capacity increases, the model reaches a point where it can exactly fit (interpolate) the training data, leading to zero training error. At this threshold, the test error typically peaks because the model is highly sensitive to training noise.</p> <p>The Second Descent: Surprisingly, increasing capacity beyond this threshold causes the test error to decrease again, often reaching a lower error than the previous \"optimal\" point. This is because larger models tend to find smoother solutions that generalize better.</p> <p></p> <p>This phenomenon is illustrated in the book using the MNIST-1D dataset, where the test error continues to decrease even after the model has reached the capacity to memorize the dataset. While classical theory suggests that highly overparameterized models should generalize poorly, the double descent curve shows that larger models can actually lead to better performance, challenging traditional notions of overfitting.</p> <p>IV - Choosing hyperparameters:</p> <p>Hyperparameters consist of not only the number of hidden layers and the number of hidden units per layer, but also of the learning rate, the choice of the learning algorithm itself, the batch size and much more. The process of finding the best hyperparameters is called hyperparameter search or neural architecture search (when focusing on the network structure).</p> <p>Hyperparameters are typically chosen empirically: we train many models with different hyperparameters on the same training dataset, then measure their performance and retain the best model. However, measuring the performance does not happen on the test set, as this may produce good results on only that specific set but the model does not generalize well on other new unseen data. Instead, we use a third dataset called the validation set. That means, for each choice of hyperparameters we train our model on the training set and evaluate its performance on the validation set and finally test the model using the testing set.</p>"},{"location":"reports/week04_ch8_9/#chapter-9-regularization","title":"Chapter 9: Regularization","text":"<p>Introduction: Why Regularization Is Needed</p> <p>Neural networks are powerful function approximators, capable of learning highly complex relationships from data. However, this expressive power can easily become a drawback. When a model has many parameters, it can fit the training data extremely closely, including random fluctuations and noise. While this often leads to very low training error, it does not necessarily translate into good performance on unseen data. This phenomenon is known as overfitting.</p> <p>Regularization addresses this issue by limiting the effective complexity of the model and encouraging solutions that generalize beyond the training set. Rather than focusing solely on minimizing training error, regularization introduces additional constraints or biases that reflect the idea that simpler models are often preferable. In practice, regularization methods guide the learning process toward parameter configurations that are more stable, less sensitive to small perturbations, and better aligned with the underlying structure of the data.</p> <p>Several regularization techniques are presented, ranging from explicit penalties added to the loss function to implicit effects arising from optimization, as well as data- and model-based approaches such as dropout,  ensembling, and data augmentation...</p> <p>1. Explicit Regularization</p> <p>Explicit regularization modifies the training objective by adding a penalty term that discourages certain parameter configurations, most commonly those involving large weight values. The optimization problem becomes: </p> <p></p> <p>Here, \u2113\ud835\udc56(\ud835\udc65\ud835\udc56,\ud835\udc66\ud835\udc56) represents the data loss, \ud835\udc54(\ud835\udf19) is a regularization function, and \ud835\udf06 controls the strength of the penalty. A common choice is L2 regularization, where \ud835\udc54(\ud835\udf19)=\u2225\ud835\udf19\u2225^2</p> <p>The intuition behind explicit regularization is that large weights tend to produce highly sensitive models, where small changes in the input can lead to large changes in the output. By penalizing large parameter values, the optimization process must balance two competing objectives: fitting the training data accurately and keeping the model parameters within a reasonable range. As \ud835\udf06 increases, the model is increasingly encouraged to favor simpler parameter configurations, even if this slightly increases training error.</p> <p>1.1 Loss Landscape Interpretation</p> <p></p> <p>A useful way to understand explicit regularization is through loss landscapes. The original loss surface may contain many sharp minima corresponding to highly specialized solutions that fit the training data very closely. The regularization term typically has a smooth, convex shape centered around zero. When the two are combined, the resulting loss surface becomes smoother and often shifts the location of the minimum toward flatter regions. These flatter minima are generally associated with better generalization, as they indicate solutions that are less sensitive to small changes in the parameters.</p> <p>1.2 Probabilistic Interpretation of Regularization</p> <p>Regularization can also be interpreted within a probabilistic framework. Training a neural network using standard loss functions corresponds to maximizing the likelihood of the observed data:</p> <p></p> <p>Introducing a regularization term is equivalent to assuming a prior distribution over the parameters. This leads to:</p> <p></p> <p>Taking the negative logarithm reveals that the regularization term corresponds to the negative log-prior. For example, penalizing large weights (L2) is equivalent to assuming that parameters are more likely to be close to zero than extremely large in magnitude. This interpretation provides a principled justification for regularization and clarifies the role of the hyperparameter \ud835\udf06 as a trade-off between fitting the data and enforcing prior assumptions.</p> <p>2. Implicit Regularization</p> <p>2.1 Gradient Descent</p> <p></p> <p>Interestingly, regularization effects can arise even when no explicit penalty is added to the loss function. Gradient descent (cyan trajectory), when implemented with a finite step size \ud835\udefc, does not follow the exact continuous trajectory (gray trajectory) implied by gradient flow. Instead, parameters are updated discretely:</p> <p></p> <p>When adding regularization term to the continuous path, it causes the optimization path to deviate from it's current trajectory to the same gray one. It can be shown that gradient descent effectively minimizes a modified loss function similar to an explicit regularization case (c). mathematically, this can be shown as :</p> <p></p> <p>The additional term penalizes regions where the gradient norm is large, meaning steep parts of the loss surface. As a result, gradient descent tends to favor flatter minima, which are known to generalize better. This phenomenon also explains why, in some cases, larger learning rates lead to improved generalization, as they strengthen this implicit regularization effect.</p> <p></p> <p>2.2 Implicit Regularization in Stochastic Gradient Descent (SGD)</p> <p>Stochastic gradient descent introduces an additional source of implicit regularization through randomness. Because gradients are computed using mini-batches, each update is only an approximation of the true gradient over the entire dataset. Different mini-batches can produce slightly different gradient directions.</p> <p>The effective loss minimized by SGD can be written as:</p> <p></p> <p>The new additional term corresponds to the variance of gradients across mini-batches. Each mini-batch gives a slightly different gradient direction. if one mini-batch points one way, and another batch points the opposite way \u21d2 poor generalization. if all batches roughly agree, that means the model fits everything reasonably well \u21d2  better generalization. meaning SGD implicitly favors solutions where all batches agree on the gradient direction, indicating a more uniform fit across the dataset.</p> <p>Consequently, SGD tends to favor parameter configurations where gradients are consistent across batches, meaning the model fits all parts of the data reasonably well rather than fitting some examples extremely well at the expense of others. This provides insight into why smaller batch sizes often lead to better generalization and why SGD frequently outperforms full-batch gradient descent in practice.</p> <p>3. Early Stopping</p> <p>Early stopping is a simple yet powerful regularization technique based on monitoring model performance during training. Initially, training reduces both training and validation error as the model learns the general structure of the data. After a certain point, continued training primarily improves performance on the training set while degrading performance on the validation set.</p> <p></p> <p>In the early stages, the model output closely follows the true underlying function. With prolonged training, the model becomes increasingly complex and overfits the noisy training points (e &amp; f).</p> <p>By monitoring the validation loss and stopping training when it stops improving, early stopping effectively limits model complexity. This technique has been shown to behave similarly to L2 regularization, as it prevents weights from growing too large.</p> <p>4. Ensembling</p> <p>Ensembling improves generalization by combining the predictions of multiple models. Instead of relying on a single trained network, the final prediction is obtained by averaging outputs or pre-softmax activations; Each individual model may overfit in a slightly different way. By averaging their predictions, random fluctuations and noise are reduced and individual errors tend to cancel out. Common ensembling strategies include:   -Training models with different random initializations   -Bagging (bootstrap aggregating)   -Using different architectures or hyperparameters</p> <p>5. Dropout</p> <p>Dropout introduces regularization by randomly deactivating neurons during training. This prevents the network from relying too heavily on specific activations and forces it to distribute information across multiple pathways.</p> <p></p> <p>By preventing neurons from relying too strongly on specific other neurons (co-adaptation), dropout forces the network to learn more robust, distributed representations. Over many iterations, sharp irregularities or undesirable kink (b) in the learned function are smoothed out.</p> <p></p> <p>8. Noise Injection</p> <p>Adding noise during training is another way to encourage robustness. Noise can be applied to inputs, parameters, or labels.</p> <p></p> <p>Adding noise forces the model to perform well despite small perturbations, discouraging overfitting. Input noise teaches invariance to irrelevant variations, weight noise encourages stability under parameter perturbations, and label smoothing prevents the model from becoming overly confident.</p> <p>9. Transferlearning</p> <p>Higher-level regularization strategies rely on sharing information across tasks or data variations. Transfer learning allows a model to reuse representations learned from a large related dataset, reducing the need to learn everything from scratch, which is particularly useful when labeled data is scarce.</p> <p>Multi-task learning trains a single model on several related tasks simultaneously, encouraging shared representations that capture common structure.</p> <p>10. Data Augmentaion</p> <p>Data augmentation increases the effective size of the training dataset by applying label-preserving transformations to existing samples. Common transformations include flipping, rotating, cropping, or adding color variations. By exposing the model to a wider range of inputs, data augmentation teaches invariance to irrelevant variations and significantly reduces overfitting.</p>"},{"location":"reports/week04_ch8_9/#discussion-notes","title":"Discussion Notes","text":""},{"location":"reports/week04_ch8_9/#measuring-performance","title":"Measuring performance","text":"<p>How do we determine if we have enough training samples?</p> <ul> <li>There is no fixed limit or specific number of samples required, it depends on the task and the dimensionality of the data.</li> <li>As the number of dimensions increases, the volume of the input space grows exponentially so the amount of data required also increases exponentially.</li> </ul> <p>What is the Generalization Gap?</p> <ul> <li>It is the difference between the model's performance on the training data and its performance on new test data.</li> </ul> <p>Why does Test Loss increase even while accuracy stabilizes or improves?</p> <ul> <li>Overconfidence, doesn't allow us to know if it's making a mistake because it's so confident.</li> <li>Softmax Effect, activations are pushed to extreme values to make the probability of the correct class higher.</li> </ul> <p>What is the Bias-Variance Trade-off? Balance between model complexity and error:</p> <ul> <li>High bias, low variance - underfitting.</li> <li>Low bias, high variance - overfitting.</li> </ul> <ul> <li> <p>What exactly is bias and variance?</p> <ul> <li>Bias: fitting is limited by the expressiveness of the model =&gt; underfitting</li> <li>Variance: prediction strongly depends on specific training set instead of averaging =&gt; overfitting</li> </ul> </li> <li> <p>Low training error doesn't guarantee low test error, but it is the hope: generalization</p> </li> <li> <p>Training increases confidence even in wrong predictions =&gt; cross-entropy can grow even while accuracy stays constant (sign of overfitting)</p> </li> <li> <p>Generalization is measured by test error or gap between training and test error</p> </li> <li> <p>Should you stop training when the testing error stays the same?</p> <ul> <li>Potentially reduces overfitting</li> </ul> </li> <li> <p>When do you have enough training/test data?</p> <ul> <li>More data generally helps, also depends on problem</li> <li>Architecture of neural network should be able to \"accommodate\" the data</li> <li>Cost factor must be considered</li> </ul> </li> <li> <p>High-dimensional problems require exponentially many samples, underlying lower-dimensional manifolds could help</p> </li> </ul>"},{"location":"reports/week04_ch8_9/#regularization","title":"Regularization","text":"<p>What is Explicit Regularization?</p> <ul> <li>Explicit regularization involves adding a term to the loss function that penalizes certain parameter values to discourage overfitting.</li> <li>Maximizing the product of Likelihood and Prior is equivalent to minimizing the sum of Loss and Regularization.</li> </ul> <p>How does L2 Regularization work?</p> <ul> <li>It adds a penalty proportional to the sum of the squared weights.</li> <li>This encourages weights to be small.</li> <li>It reduces variance but increases bias. In over-parameterized models.</li> </ul> <ul> <li> <p>Gaussian prior: belief that small parameters are more probable; applies to hypothesis space not directly to data</p> </li> <li> <p>How does regularization come into the bias-variance-tradeoff?</p> <ul> <li>Increases bias while reducing variance and overfitting risk (depends on type)</li> </ul> </li> <li> <p>Data augmentation increases complexity despite producing samples that lie in the same manifold</p> </li> <li> <p>Why use L2-regularization, especially in comparison to L1-regularization?</p> <ul> <li>L1 promotes zero weights while L2 penalizes large weights smoothly</li> </ul> </li> </ul>"},{"location":"reports/week04_ch8_9/#references","title":"References","text":"<ul> <li>IBM - What is overfitting ? [Link]</li> <li>Three Sources of Model Error: Bias, Variance, and Noise [Link]</li> <li>A Modern Take on the Bias-Variance Tradeoff in Neural Networks [Link]</li> <li>The Bias-Variance trade-off [Link]</li> <li>The Importance of Data Splitting [Link]</li> <li>Deep Double Descent: where bigger models and more data hurt [Link]</li> </ul>"},{"location":"reports/week05_ch7/","title":"Report \u2013 Week 5: Gradients &amp; Initialization","text":"<p>Presenter: Noah G\u00f6vert  </p> <p>Date: 17.11.2025 </p>"},{"location":"reports/week05_ch7/#summary","title":"Summary","text":"<p>Now that we know how neural networks are structured, how we can apply loss functions to them, and how to minimize  functions based on derivatives using methods like gradient descent, we will take a look at how to efficiently compute  the derivatives of our neural network with the applied loss function with respect to its parameters. This will allow us  to finally optimize our nets.</p>"},{"location":"reports/week05_ch7/#observations","title":"Observations","text":"<p>The derivative of the loss function with respect to a certain parameter $\\omega$ basically describes how a tiny change  to this specific parameter would affect the output of the loss function. Based on that and the structure of a simple  neural network as shown in Fig. 1 we can make the following two observations on how to compute the derivatives  of the loss function with respect to the network's parameters. 1. The weights are always multiplied with the activations of a hidden unit and then added to the pre-activations of the  following layer (blue arrow in Fig. 1). Therefore, we will save the activations during the computation of the loss  function's output, which is also called the forward pass. 2. Small changes to weights or biases in early layers can cause a ripple effect through the whole network and  completely alter the result. So in order to know how the change of a weight affects the loss function, we also need to  know how all the subsequent layers change the output. We will quantize this effect of the subsequent layers as the  upstream gradient and reuse this gradient to compute the upstream gradient for the previous layer and so forth. This is  done during the so-called backward pass.</p> <p>So in summary, we will compute the gradient by strategically saving and reusing important intermediate values during the forward and backward pass. </p>"},{"location":"reports/week05_ch7/#a-toy-example","title":"A toy example","text":"<p>In order to apply this intuition about gradient computation in neural networks we will now consider a simple 3 layer  network $f[x,\\phi]$ with least squares loss and a layer width of one. The $\\sin$ function, the $\\exp$ function and the  $\\cos$ function can be considered the activation functions. $$ \\ell_i[f[x,\\phi],y_i]=[\\beta_3+\\omega_3\\cdot\\cos[\\beta_2+\\omega_2\\cdot\\exp[\\beta_1+\\omega_1\\cdot\\sin[\\beta_0+\\omega_0\\cdot x]]]-y_i]^2 $$</p>"},{"location":"reports/week05_ch7/#forward-pass","title":"Forward pass:","text":"<p>We can rewrite the loss function as the following chain of computations, where $f_i$ denotes the preactivations and  $h_i$ the activations: $$ \\begin{aligned} f_0 &amp;= \\beta_0 + \\omega_0 \\cdot x \\ h_1 &amp;= \\sin[f_0] \\ f_1 &amp;= \\beta_1 + \\omega_1 \\cdot h_1 \\ h_2 &amp;= \\exp[f_1] \\ f_2 &amp;= \\beta_2 + \\omega_2 \\cdot h_2 \\ h_3 &amp;= \\cos[f_2] \\ f_3 &amp;= \\beta_3 + \\omega_3 \\cdot h_3 \\ \\ell_i &amp;= [f_3 - y_i]^2 \\end{aligned} $$ In the forward pass we will then calculate the loss and save all the intermediate values from the chain of computations  above.</p>"},{"location":"reports/week05_ch7/#backward-pass-1","title":"Backward pass #1:","text":"<p>Now we will compute the derivatives with respect to our intermediate values in reverse order. Later we will then reuse  these to calculate the derivatives with respect to our parameters. The first derivative is straightforward: $$ \\frac{\\partial \\ell_i}{\\partial f_3} = 2(f_3 - y_i) $$ We can then go on and reuse this derivative in order to compute the derivative of our loss function with respect to  $f_3$ using the chain rule: $$ \\frac{\\partial \\ell_i}{\\partial h_3} = \\frac{\\partial f_3}{\\partial h_3}\\frac{\\partial \\ell_i}{\\partial f_3} $$ Then again, we can reuse the derivative computed above to calculate the derivative of the loss function with respect  to $f_2$. And just like that, by always reusing the previous derivative with the chain rule, we can progress through  the whole computation chain backwards until we get the derivative with respect to $f_0$. In the following computation  the part in braces is always already fully computed one step before: $$ \\begin{aligned} \\frac{\\partial \\ell_i}{\\partial f_2} &amp;= \\frac{\\partial h_3}{\\partial f_2}(\\frac{\\partial f_3}{\\partial h_3}\\frac{\\partial \\ell_i}{\\partial f_3}) \\ \\frac{\\partial \\ell_i}{\\partial h_2} &amp;= \\frac{\\partial f_2}{\\partial h_2}(\\frac{\\partial h_3}{\\partial f_2}\\frac{\\partial f_3}{\\partial h_3}\\frac{\\partial \\ell_i}{\\partial f_3}) \\ \\frac{\\partial \\ell_i}{\\partial f_1} &amp;= \\frac{\\partial h_2}{\\partial f_1}(\\frac{\\partial f_2}{\\partial h_2}\\frac{\\partial h_3}{\\partial f_2}\\frac{\\partial f_3}{\\partial h_3}\\frac{\\partial \\ell_i}{\\partial f_3}) \\ \\frac{\\partial \\ell_i}{\\partial h_1} &amp;= \\frac{\\partial f_1}{\\partial h_1}(\\frac{\\partial h_2}{\\partial f_1}\\frac{\\partial f_2}{\\partial h_2}\\frac{\\partial h_3}{\\partial f_2}\\frac{\\partial f_3}{\\partial h_3}\\frac{\\partial \\ell_i}{\\partial f_3}) \\ \\frac{\\partial \\ell_i}{\\partial f_0} &amp;= \\frac{\\partial h_1}{\\partial f_0}(\\frac{\\partial f_1}{\\partial h_1}\\frac{\\partial h_2}{\\partial f_1}\\frac{\\partial f_2}{\\partial h_2}\\frac{\\partial h_3}{\\partial f_2}\\frac{\\partial f_3}{\\partial h_3}\\frac{\\partial \\ell_i}{\\partial f_3}) \\ \\end{aligned} $$</p>"},{"location":"reports/week05_ch7/#backward-pass-2","title":"Backward pass #2:","text":"<p>Now that we computed all intermediate results and gradients in the forward and backward pass, we can finally compute  the derivatives with respect to our weights $\\omega$ and biases $\\beta$. This again can be done using the chain rule: $$ \\begin{aligned} \\frac{\\partial \\ell_i}{\\partial \\omega_k} &amp;= \\frac{\\partial f_k}{\\partial \\omega_k}\\frac{\\partial \\ell_i}{\\partial f_k} \\ \\frac{\\partial \\ell_i}{\\partial \\beta_k} &amp;= \\frac{\\partial f_k}{\\partial \\beta_k}\\frac{\\partial \\ell_i}{\\partial f_k} \\end{aligned} $$ Notice that the right-hand side is just the upstream gradient computed in the step before. And the left-hand side is easily computed: $$ \\begin{aligned} \\frac{\\partial f_k}{\\partial \\omega_k} &amp;= h_k \\ \\frac{\\partial f_k}{\\partial \\beta_k} &amp;= 1 \\end{aligned} $$ With the exception for $k=0$: $$ \\frac{\\partial f_k}{\\partial \\omega_k} = x_i $$</p> <p>This idea can also be extended to neural networks with a layer width of more than one. The central difference in that  case would be that all computations are then done with vectors and matrices instead of scalars, but the general process  remains the same.</p>"},{"location":"reports/week05_ch7/#algorithmic-differentiation","title":"Algorithmic differentiation","text":"<p>The idea of treating formulas as a series of computations to then compute the derivatives of this formula with respect  to each parameter by just applying the chain rule over and over again can be generalized to computational graphs. As  long as a formula is transformable to a non-circular graph with the operators on its nodes, it is possible to  automatically compute the derivatives with respect to all parameters. However, the derivatives for the operators on the  graph's nodes have to be implemented beforehand. This leads to the practical advantage, that when creating neural  networks in modern frameworks like PyTorch and TensorFlow, it usually is not necessary to implement derivatives at all,  since standard derivatives, like addition, multiplication, and so forth, are already implemented by default.</p>"},{"location":"reports/week05_ch7/#initializing-weights","title":"Initializing weights","text":"<p>Now that we fully understand how to optimize our networks from a certain starting point in the loss landscape, we still  need to find a strategy to choose reasonable starting points. Hence, we need to find a reasonable way on how to  initialize the network's weights and biases. One simple option would be to just randomly choose a single number, maybe  even zero, and set all weights and biases to this specific number. Doing this, however, would lead to symmetries in the  optimization process, and we would likely not be able to utilize the whole network's capacity. Another more practical  approach would be to sample each weight independently of a standard normal distribution. Even though this is way closer  to state-of-the-art weight sampling methods, it still leads to some problems.</p>"},{"location":"reports/week05_ch7/#exploding-and-vanishing-gradient-problem","title":"Exploding and vanishing gradient problem","text":"<p>Assuming we have a neural network with ReLU activations, and we initialize our weights according to a standard normal  distribution with $\\mu = 0$ and a really small variance $\\sigma^2$. If we would now propagate inputs through this  layer, the average magnitude of the inputs for the next layer is likely smaller than the magnitude of the inputs of the  layer before, since all inputs got multiplied with really small numbers, and then even all values less than zero got  clipped by the ReLU function. If this happens consecutively in a lot of layers, the numbers might get so small that  usual floating point precision does not have the capacity to represent these numbers accurately anymore. Something  similar happens if we initialize our weights with a big variance, but in this case the magnitude of our values would  rise uncontrollably from layer to layer instead of shrink, since the layer inputs are always multiplied with really big  numbers.</p> <p>The problem even increases during optimization in the backward pass, since the gradients here also get multiplied with  the weight matrices, which leads to the so-called vanishing or exploding gradient problem. In Fig. 2 the development of magnitude from the intermediate values in the forward pass and the gradients is shown for a network with 50 hidden  layers, each with a width of 100 hidden units. </p>"},{"location":"reports/week05_ch7/#he-initialization","title":"HE Initialization","text":"<p>As explained before, we have to be very careful with initializing our weights for big neural networks. In this section  we will introduce an initialization method that samples weights from a standard normal distribution but adjusts the  variance for each layer with the goal of keeping the magnitude of the preactivations the same for each layer.</p> <p>We will find that the variance of pre activations $\\sigma^2_{f'i}$ in a subsequent layer in the forward pass of a  simple feedforward network with ReLU activations depends as follows on the variance of the weights $\\sigma\\Omega^2$,  the variance of pre activations from its previous layer $\\sigma_f^2$ and the number of hidden units $D_h$ in its  previous layer: $$ \\sigma^2_{f'i}  = \\frac{1}{2} D_h \\, \\sigma\\Omega^2 \\, \\sigma_f^2 $$ In order to keep the variance of pre-activations the same in both layers during the forward pass we can then just set  the sampling variance for our weights as follows: $$ \\sigma^2 = \\frac{2}{D_h} $$</p>"},{"location":"reports/week05_ch7/#initialization-for-backward-pass","title":"Initialization for backward pass","text":"<p>During the backward pass, a very similar argument can be made, with the main difference being that the weights are now  multiplied with the transposed weight matrices. So in order to avoid vanishing or exploding gradients, we would have to  divide by the dimensionality of the layer the weights feed into instead of the dimensionality of the layer the weights  come from.  $$ \\sigma^2 = \\frac{2}{D_{h'}} $$ This initialization method, however, is directly in conflict with our optimal initialization method for the forward pass,  if not all layers have the same size. One common option to tackle this problem is to just take the mean of both  methods. $$ \\sigma_\\Omega^2 = \\frac{4}{D_h + D_{h'}} $$</p>"},{"location":"reports/week05_ch7/#discussion-notes","title":"Discussion Notes","text":"<p>Q: Is there a difference between computing the gradient for the combined loss (like it is done in Pytorch) vs. computing it \"point-wise\"? \\ A: No, since mathematically there is no difference whether you differentiate first or you average first.</p> <p>Q: How would you determine that your parameters were initialized poorly? \\ A: You either get an exploding gradient or vanishing gradient. An exploding gradient may be visible in the loss because the loss becomes very volatile. A vanishing gradient might make it look like the loss is not changing at all anymore.</p> <p>Q: In theory, what would happen if we intialized all weights with identical values? \\ A: If all weights were the same, all the gradients would also be identical, meaning you would not be able to distinguish between neurons and the model could not describe anything useful.</p> <p>Q: Regarding the formula for optimal initialization weights, where did the 2 come from? \\ A: The 2 originally came from the observation that the ReLU discards half the values for \u03bc=0, meaning it also halves the variance from one layer to the next.</p> <p>Q: What is the difference between static and dynamic computation graphs in ML frameworks (i.e. Tensorflow vs. Pytorch)? \\ A: In general, Tensorflow is faster, while Pytorch is more transparent, which is why researchers often use Pytorch. </p> <p>Q: We heard that backpropagation saves a lot of computation when computing the gradients. Are there also costs/ disadvantages to this approach? \\ A: Since backpropagation saves many intermediate values for its next steps, it is not particularly memory efficient. There are approaches to cope with this, like only caching the most important values and re-computing other ones to save space, but it is always a space-time-tradeoff.</p> <p>Q: The book said that one of the reasons that initialization is a potential problem is because of floating point precision. In theory, what would happen if we had infinite precision/ compute? \\ A: At this point, the question becomes \"Can I find the global minimum from any starting point?\", which should be possible with infinite compute.</p>"},{"location":"reports/week05_ch7/#references","title":"References","text":"<ul> <li>Understand Deep Learning</li> </ul>"},{"location":"reports/week06_ch12/","title":"Report \u2013 Week 06: Transformers","text":"<p>Presenter: Fadi Dalbah Date: 01.12.2025  </p>"},{"location":"reports/week06_ch12/#summary","title":"Summary","text":""},{"location":"reports/week06_ch12/#1-motivation","title":"1. Motivation","text":"<ul> <li>Text as input</li> <li>Texts can be very long.</li> <li>Variable length: each text has a different number of inputs.</li> <li>Ambiguity: the meaning of a word depends strongly on its context.</li> <li>Goal: A mechanism that can flexibly relate any input to any other in the sequence and share parameters between similar inputs.</li> </ul>"},{"location":"reports/week06_ch12/#2-self-attention","title":"2. Self-Attention","text":"<ul> <li>Think of self-attention as routing information:</li> <li>For each position, the model decides how much to take from each other position.</li> <li>$n^{th}$ output at position is the weighted sum of $N$ value vectors:</li> <li>Different outputs can use different weight distributions, i.e. focus on different parts of the sequence.</li> </ul>"},{"location":"reports/week06_ch12/#31-values-queries-and-keys","title":"3.1 Values, queries and keys","text":"<ul> <li>Each input token embedding $x_m$ is linearly mapped to a value:   $$   v_m = \\beta_v + \\Omega_v x_m   $$</li> <li>The same input is also mapped to a query and a key:   $$   q_n = \\beta_q + \\Omega_q x_n   $$   $$   k_m = \\beta_k + \\Omega_k x_m   $$</li> </ul>"},{"location":"reports/week06_ch12/#32-attention-scores-and-weights","title":"3.2 Attention scores and weights","text":"<ul> <li>Similarity score between token $m$ and position $n$ via dot product:   $$   e_{mn} = k_m^T q_n   $$</li> <li>Convert scores into attention weights with a softmax over $m$:   $$   a[x_n, x_m] = \\text{softmax}m(e{mn})   $$   $$   = \\frac{\\exp(e_{mn})}{\\sum_{m'=1}^N \\exp(e_{m'n})}   $$</li> </ul>"},{"location":"reports/week06_ch12/#33-output-of-self-attention","title":"3.3 Output of self-attention","text":"<ul> <li>Per position $n$:   $$   \\text{sa}n([x_1,\\dots,x_N]) = \\sum{m=1}^N a[x_m, x_n] \\, v_m   $$</li> <li>Matrix notation:</li> <li>Compute:     $$     Q = \\Beta_q1^T + \\Omega_q X     $$     $$     K = \\Beta_q1^T + \\Omega_k X     $$     $$     V = \\Beta_q1^T + \\Omega_v X     $$</li> <li>Basic self-attention:     $$     \\text{Sa}(X) = V \\cdot \\text{Softmax}(K^T Q)     $$</li> </ul>"},{"location":"reports/week06_ch12/#4-important-extensions-of-self-attention","title":"4. Important Extensions of Self-Attention","text":""},{"location":"reports/week06_ch12/#41-positional-encoding","title":"4.1 Positional encoding","text":"<ul> <li>Self-attention alone is order-invariant.</li> <li>Add a positional encoding to each token:</li> <li>$p_n$ can be:</li> <li>Absolute (depends on position index) <ul> <li>Chosen or learned,</li> </ul> </li> <li>Relative (depends on distances between positions),</li> </ul>"},{"location":"reports/week06_ch12/#42-scaled-dot-product-attention","title":"4.2 Scaled dot-product attention","text":"<ul> <li>In high dimensions, dot products can become large.</li> <li>This can cause:</li> <li>Largest value dominates softmax,</li> <li>Small gradient changes \u2192 harder training.</li> <li>Solution: scale the scores by query dimension $\\sqrt{D_q}$:   $$   \\text{SA}(X) = V \\cdot \\text{Softmax}!\\left(\\frac{K^T Q}{\\sqrt{D_q}}\\right)   $$</li> </ul>"},{"location":"reports/week06_ch12/#43-multi-head-attention","title":"4.3 Multi-head attention","text":"<ul> <li>Run multiple self-attentions in parallel:</li> <li>Each head $h$ has its own computation.</li> <li>Compute:     $$     Q_h = \\Beta_{vh}1^T + \\Omega_{kh} X     $$     $$     K_h = \\Beta_{qh}1^T + \\Omega_{kh} X     $$     $$     V_h = \\Beta_{kh}1^T + \\Omega_{kh} X     $$</li> <li>Concatenate and linearly transform:   $$   \\text{Sa}_h(X) = V_h \\cdot \\text{Softmax}!\\left(\\frac{K^T_h Q_h}{\\sqrt{D_q}}\\right)   $$</li> <li>Can make network more robust to bad initializations</li> </ul>"},{"location":"reports/week06_ch12/#5-transformers-for-nlp","title":"5. Transformers for NLP","text":""},{"location":"reports/week06_ch12/#51-tokenization-and-embeddings","title":"5.1 Tokenization and embeddings","text":"<ul> <li>Tokenization:</li> <li>Split text into subword tokens from a fixed vocabulary.</li> <li>Embedding:</li> <li>Each token index is mapped to a dense vector (word embedding).</li> </ul>"},{"location":"reports/week06_ch12/#52-encoder-only-model-bert-example-configuration","title":"5.2 Encoder-only model: BERT (example configuration)","text":"<ul> <li>Input: full sentence \u2192 bidirectional self-attention.</li> <li>Typical configuration presented:</li> <li>Vocabulary: ~30\u202f000 tokens.</li> <li>Embedding dimension: 1\u202f024.</li> <li>24 transformer layers, each with 16 heads.</li> <li>Query/key/value projections: $64 \\times 1\\,024$ per head.</li> <li>Feed-forward hidden dimension: 4\u202f096.</li> <li>\u2248 340M parameters.</li> <li>For pretaining</li> <li>Inputs are converted to embeddings</li> <li>Passed through transformer layers</li> <li>Small fraction of tokens replaced with <code>&lt;mask&gt;</code> token</li> <li>Goal is to predict the right token</li> <li>For classification</li> <li><code>&lt;cls&gt;</code> token placed at the start of string</li> <li>Token mapped to a number</li> </ul>"},{"location":"reports/week06_ch12/#6-decoder-only-models-and-masked-attention-gpt-3-style","title":"6. Decoder-only Models and Masked Attention (GPT-3 style)","text":""},{"location":"reports/week06_ch12/#61-autoregressive-objective","title":"6.1 Autoregressive objective","text":"<ul> <li>Model predicts the next token:   $$   Pr(t_1, t_2, \\dots, t_N)   $$</li> <li>This defines a probability for the whole sequence:   $$   Pr(t_1, t_2, \\dots, t_N) = Pr(t_1)\\prod_{n=2}^N Pr(t_n \\mid t_1, \\dots, t_{n-1})   $$</li> </ul>"},{"location":"reports/week06_ch12/#62-masked-self-attention","title":"6.2 Masked self-attention","text":"<ul> <li>During training, token at position $n$ must not see future tokens.</li> <li>Implemented by adding a mask matrix</li> <li>Entries with $-\\infty$ become 0 after softmax.</li> </ul>"},{"location":"reports/week06_ch12/#63-generating-text","title":"6.3 Generating text","text":"<ul> <li>Start with a special <code>&lt;start&gt;</code> token.</li> <li>Repeatedly:</li> <li>Compute distribution over next token via masked self-attention and output layer.</li> <li>Sample or choose the most probable token.</li> <li>Append it to the sequence and feed back into the decoder.</li> <li> <p>Stop when <code>&lt;end&gt;</code> token is produced.</p> </li> <li> <p>Large decoder models support few-shot learning:</p> </li> <li>A few examples in the prompt are enough to perform a new task without changing the model parameters.</li> </ul>"},{"location":"reports/week06_ch12/#7-encoderdecoder-transformers-and-cross-attention","title":"7. Encoder\u2013Decoder Transformers and Cross-Attention","text":"<ul> <li>Encoder:</li> <li>Processes source sentence, producing context representations.</li> <li>Decoder:</li> <li>Uses:<ul> <li>Masked self-attention over previous target tokens.</li> <li>Cross-attention over encoder states:</li> <li>Queries $Q$ from decoder embeddings,</li> <li>Keys $K$ and values $V$ from encoder outputs $E$.</li> </ul> </li> <li> <p>Same scaled dot-product formula, just with different sources for $Q$ vs. $K, V$.</p> </li> <li> <p>Main application: machine translation.</p> </li> </ul>"},{"location":"reports/week06_ch12/#8-variants","title":"8. Variants","text":"<ul> <li>Long-sequence transformers (efficient attention for long texts).</li> <li>Image transformer and ImageGPT (apply attention to image patches).</li> <li>Vision Transformer (ViT) and multi-scale ViT (hierarchical image representations).</li> <li>Many other adaptations to different data types and tasks.</li> </ul>"},{"location":"reports/week06_ch12/#discussion-notes","title":"Discussion Notes","text":""},{"location":"reports/week06_ch12/#sampling-methods","title":"Sampling Methods","text":"<p>When generating text from a Large Language Model we always try to predict the most likely token given the preceding  text sequence. However, just outputting the token with the highest probability is not always the best option. For  example, in a scenario, where the cumulative probability for a group of words that points in a similar direction is  bigger than the cumulative probability for a group of words that points in a different direction, but the word with  the biggest associated probability is from the second group and thus would be sampled and lead the generation in a wrong direction. This problem is also already known in other fields, like in elections, as  Spoiler effect.</p> <p>A common solution to tackle this problem is Top-k sampling. Here, instead of just always outputting the token with  the highest probability, we sample from the k most likely tokens according to their probability. Another similar  approach would be Top-p sampling, where $p$ is a probability, and we sample from the tokens with the biggest  probability that cumulate to least $p$ of the total probability from all options. With for example  Beam search there are also different approaches that keep track of sequences with the highest probabilities, and  thus try to predict the optimal sequence of subsequent tokens.</p>"},{"location":"reports/week06_ch12/#attention-for-long-sequences","title":"Attention for long sequences","text":"<p>In attention, each token interacts with every other token from the sequence. This leads to a quadratic complexity of the  attention mechanism, and thus the attention computation for very long sequences takes up a lot of resources. However,  there are some methods developed to tackle this problem. Most approaches sparsify the attention interaction matrix for  example through a convolutional structure (Fig. 1c-f). The tradeoff here, however, is that tokens can only interact  with some of the other tokens through the course of several subsequent layers. This problem can be partially tackled by  introducing some tokens that attend to all other tokens (Fig. 1g). </p>"},{"location":"reports/week06_ch12/#why-are-positional-encodings-added-to-the-tokens-and-not-concatenated-with-them","title":"Why are positional encodings added to the tokens and not concatenated with them?","text":"<p>Adding positional encodings keeps the dimensionality of the model smaller and makes it easier to alter already existing  models, since the dimensionality does not have to change, while the model can still reasonably distinguish between  positional and token information.</p>"},{"location":"reports/week06_ch12/#how-big-are-modern-large-language-models-llms","title":"How big are modern Large Language Models (LLMs)?","text":"<p>Modern LLMs can have way more than a hundred billion parameters. Some of them even exceed the mark of a trillion parameters. For  example Metas Llama 4 Maverick has a total of 400 billion  parameters. In order to run this model with 16-bit floating point precision, you would need around 800 GB of GPU RAM.</p>"},{"location":"reports/week06_ch12/#challenges-of-transformers-in-computer-vision-tasks","title":"Challenges of transformers in computer vision tasks","text":"<p>Images have a lot of pixels, which poses a problem, because the attention matrix grows quadratically with the number of  inputs. Also, convolutional networks are particularly well suited for the two-dimensional structure of images. However,  because of the massive number of training datapoints and the increase in compute resources transformer models have now  eclipsed the performance of convolutional networks in many computer vision tasks.</p>"},{"location":"reports/week06_ch12/#references","title":"References","text":"<ul> <li>Understand Deep Learning</li> <li>Spoiler effect</li> <li>Metas Llama 4 Maverick</li> </ul>"},{"location":"reports/week07_ch13/","title":"Report - Week 7, Chapter 13: Graph Neural Networks","text":"<p>Presentation Date: 08.12.2025\\ Presenter: Minh Nhat Vu\\ Discussion Lead: Aisaiah Pellecer\\ Report by: Minh Nhat Vu</p>"},{"location":"reports/week07_ch13/#summary","title":"Summary","text":""},{"location":"reports/week07_ch13/#motivation","title":"Motivation","text":"<p>Graph Neural Networks are neural architectures that process graphs. However, this is a challenging task because graphs can have a variable topology and can be very large and densely connected. In addition, only a single graph is available, which prevents the usual training on many data examples and testing with new data.</p>"},{"location":"reports/week07_ch13/#graph-representation","title":"Graph representation","text":"<p>Graphs consist of a set of nodes $N$ and edges $E$ that connect pairs of nodes and represent real-world objects such as road networks or electrical circuits. They can be directed or undirected and may also be multigraphs, allowing multiple edges between two nodes.</p> <p>A graph structure is represented by an adjacency matrix $\\mathbf{A}$, with entries indicating whether an edge should exist between two nodes. Additionally, a graph can store information via a node data matrix $\\textbf{X}$ or an edge data matrix $\\textbf{E}$. Both contain concatenated fixed-length vectors for each node or edge.</p>"},{"location":"reports/week07_ch13/#graph-neural-networks","title":"Graph neural networks","text":"<p>Graph neural networks are models that take node embeddings and adjacency matrix as input and are processed through a series of $K$ layers. At the beginning, In each layer, the node embedding is updated which are defined as hidden representations $\\mathbf{H}_K$. As a result, the model output contains information about the nodes and their context in the graph.</p>"},{"location":"reports/week07_ch13/#graph-level-tasks","title":"Graph-level tasks","text":"<p>Supervised graph problems are subdivided into graph-level tasks, node-level tasks and edge-level tasks. Graph-level tasks assign a label or estimate one or more values for the entire graph such as identifying the temperature at which a molecule becomes liquid. Node-level tasks assign one or more values to each node of the graph, based on the graph structure and the node embeddings. Edge-level tasks predict whether an edge should exist between two nodes, for example, in a social network to determine if two people know each other.</p>"},{"location":"reports/week07_ch13/#graph-convolutional-networks","title":"Graph convolutional networks","text":"<p>Spatial-based graph convolutional networks (GCNs) update each node by aggregating information from its neighboring nodes at each layer. Formally, this process can be represented by a function $\\mathbf{F[\\bullet]}$ with parameters $\\Phi$  that takes the node embeddings and the adjacency matrix as input and outputs a new node embedding. This can be written as:</p> <p>$$ \\begin{aligned} \\mathbf{H}1 &amp;= \\mathbf{F}[\\mathbf{X}, \\mathbf{A}, \\phi_0] \\ \\ \\mathbf{H}_2 &amp;= \\mathbf{F}[\\mathbf{H_1}, \\mathbf{A}, \\phi_1] \\ \\ \\mathbf{H}_3 &amp;= \\mathbf{F}[\\mathbf{H_2}, \\mathbf{A}, \\phi_2] \\ \\ &amp;\\vdots \\ \\ \\mathbf{H}_K &amp;= \\mathbf{F}[\\mathbf{H{k-1}}, \\mathbf{A}, \\phi_{k-1}] \\ \\end{aligned} $$</p> <p>The nodes are arbitrarily indexed, making each layer equivariant: permuting the node indices also permutes the node embeddings. The output is invariant to the node order, so permuting the node indices does not affect it. An example application for a GCN is the classification of molecules as either harmless or toxic.</p>"},{"location":"reports/week07_ch13/#inductive-vs-transductive-models","title":"Inductive vs. transductive models","text":"<p>Inductive models uses a training set of labeled data to learn the relationship between the input and the output. This relationship is then applied to new, unseen test data. In contrast, transductive models use both labeled and unlabeled data simultaneously and produce only labels for the unlabeled data rather than as a general rule.</p> <p>As an example binary node classification is transductive: the goal is to label the remaining unlabeled nodes from a large graph. However, two problems arise: The graph cannot be stored logistically due to its size and having only a single graph makes the application of a stochastic gradient descent difficult.</p>"},{"location":"reports/week07_ch13/#choosing-batches","title":"Choosing batches","text":"<p>To address this, a subset of nodes is selected as a batch. For each of node, its receptive field (k-hop neighborhood) is considered allowing to get input from nearby nodes in the previous layer. A gradient descent step is applied over the union of the k-hop neighborhoods of the nodes, while the remaining nodes do not contribute. However, with a densely connected graph and many layers, the size remains large.</p> <p>Two approaches address this: First, neighbor sampling uses a random subset of neighbor nodes as input, with a fixed number of neighbors. Second, graph partitioning divides the graph into smaller subgraphs that are used as training batches in a transductive setting.</p>"},{"location":"reports/week07_ch13/#layers-for-graph-convolutional-networks","title":"Layers for graph convolutional networks","text":"<p>There are several ways to aggregate the information from the nodes for the layers in a GCN. One method is that the current node gathers information from its neighbors by summing them up. Another approach is to take the average instead of the sum to make the aggregation independent of the number of neighbors. These aggregation methods depend on the graph topology and the weight of the contribution of the neighbors equally.</p> <p>In contrast, there exists a graph attention layer where the weights depend on the data at the nodes. Such layers are used in a graph attention network. The basic idea is as follows: a linear transformation is applied to the current node embedding. Then, the similarity to every node is computed by using a pre attention matrix. In the end, an attention matrix is applied to ensure that only the values of the node itself and its neighbors contribute to the node embedding.</p>"},{"location":"reports/week07_ch13/#edge-graphs","title":"Edge graphs","text":"<p>To process information for edge embeddings instead of node embeddings, the graph is transformed into its edge graph by placing a node on each original edge and connecting these new nodes if they share an original node. The original nodes are removed during the process.</p>"},{"location":"reports/week07_ch13/#report-week-7-chapter-13-graph-neural-networks_1","title":"Report - Week 7, Chapter 13: Graph Neural Networks","text":"<p>Presentation Date: 08.12.2025\\ Presenter: Nhat Vu Minh\\ Discussion Lead: Aisaiah Pellecer\\ Report by: Aisaiah Pellecer</p>"},{"location":"reports/week07_ch13/#graph-representation_1","title":"Graph Representation","text":"<p>A graph consists of a set of nodes connected by edges. Both nodes and edges may carry information in the form of node embeddings and edge embeddings. </p> <p>Graphs are a natural way to represent structured data where relationships between entities are important, such as social networks, citation graphs, and molecular structures.</p> <p>In some cases, edges themselves can be treated as nodes by constructing an edge graph, which allows graph neural networks to update edge embeddings using the same message-passing mechanisms applied to nodes.</p>"},{"location":"reports/week07_ch13/#graph-neural-networks-gnns","title":"Graph Neural Networks (GNNs)","text":"<p>A graph neural network is a model that takes the node embeddings X and the adjacency matrix A as inputs and processes them through a sequence of K layers. At each layer, node embeddings are updated, producing intermediate hidden representations $H_{k}$, until final output embeddings $H_{K}$ are obtained.</p> <p>Initially, each node embedding contains information only about the node itself. As embeddings pass through successive layers, information from neighboring nodes is incorporated through message passing. By the final layer, each node embedding captures both the node\u2019s own features and its context within the graph.</p> <p>This process is analogous to word embeddings in transformer models: while input embeddings represent individual words in isolation, the output embeddings represent word meanings conditioned on their surrounding context.</p> <p>Since node ordering in graphs is arbitrary, GNN layers must be permutation equivariant with respect to node indices. Parameter sharing across nodes enables generalization across graphs of different sizes and structures.</p>"},{"location":"reports/week07_ch13/#tasks-in-graph-neural-networks","title":"Tasks in Graph Neural Networks","text":"<p>Graph neural networks can be applied to three main types of tasks:</p> <ul> <li> <p>Edge prediction tasks:   The model predicts whether an edge should exist between two nodes. This is commonly used for link prediction and recommendation problems.</p> </li> <li> <p>Node-level tasks:   The model assigns a label (classification) or continuous values (regression) to each node. Predictions depend on both node embeddings and the graph structure, allowing nodes to be interpreted in context.</p> </li> <li> <p>Graph-level tasks:   The model assigns a label or predicts one or more values for the entire graph, exploiting global structural information and node embeddings.</p> </li> </ul> <p>Node- and edge-level tasks require permutation-equivariant outputs, while graph-level tasks require permutation-invariant functions.</p>"},{"location":"reports/week07_ch13/#spatial-based-graph-convolutional-networks-gcns","title":"Spatial-Based Graph Convolutional Networks (GCNs)","text":"<p>Spatial-based graph convolutional networks update node representations by aggregating information from neighboring nodes in the original graph. They are referred to as convolutional because the same local aggregation rule is applied at every node.</p> <p>These models induce a relational inductive bias, favoring information from nearby nodes, and are considered spatial-based because they operate directly on the given graph structure rather than transforming the graph into another domain.</p>"},{"location":"reports/week07_ch13/#inductive-vs-transductive-models_1","title":"Inductive vs. Transductive Models","text":"<p>Graph-level tasks occur exclusively in the inductive setting, where the model is trained on a set of graphs and evaluated on unseen graphs.</p> <p>Node-level and edge prediction tasks can occur in both settings:</p> <ul> <li> <p>In the transductive setting, learning takes place on a single large graph with partial labels. The loss function is computed only where ground truth is known, while predictions for unlabeled nodes or edges are obtained by running a forward pass and reading out the corresponding outputs. Unlabeled nodes still influence learning through message passing.</p> </li> <li> <p>In the inductive setting, the model is trained on multiple graphs or subgraphs and can generalize to unseen nodes or graphs. Partitioning large graphs into subgraphs can effectively convert a transductive problem into an inductive one.</p> </li> </ul>"},{"location":"reports/week07_ch13/#layers-for-graph-convolutional-neural-networks-gcnns","title":"Layers for Graph Convolutional Neural Networks (GCNNs)","text":"<p>A typical GCNN layer consists of: 1. Aggregation of neighbor information (mean, sum, or max) 2. Combination with the node\u2019s current embedding 3. Non-linear transformation using shared parameters</p> <p>Stacking multiple layers increases the receptive field (region of the graph that contributes to a given node), allowing nodes to incorporate information from multi-hop neighborhoods of batch nodes-- think of this as neighborhood sampling or graph partitioning. </p>"},{"location":"reports/week07_ch13/#discussion-notes","title":"Discussion Notes","text":""},{"location":"reports/week07_ch13/#key-questions-and-responses","title":"Key Questions and Responses","text":"<p>Why are graphs in a batch treated as disjoint components of a single large graph? This allows efficient batching and parallel computation. Since message passing only occurs along edges, disconnected graphs do not exchange information. However, unbalanced graphs may dominate gradient updates and affect training stability.</p> <p>How is unlabeled data used in transductive learning? Unlabeled nodes participate in message passing and influence node embeddings, acting as a form of semi-supervised learning. Class imbalance within a single large graph can introduce bias.</p> <p>What is the receptive field in GNNs? The receptive field corresponds to the k-hop neighborhood of a node, where k is the number of GNN layers.</p> <p>Do unbalanced partitions affect training? Yes. Large or dense subgraphs can dominate gradients. Further partitioning improves efficiency but may increase bias by removing long-range dependencies.</p> <p>What is diagonal enhancement? Diagonal enhancement adds self-loops to preserve node identity and stabilize message passing.</p> <p>Why use mean aggregation? Mean aggregation normalizes by neighborhood size, leading to more stable training and better inductive generalization across graphs with varying node degrees.</p>"},{"location":"reports/week07_ch13/#references","title":"References","text":"<ul> <li>Deep Learning using Rectified Linear Units (ReLU) [Link]</li> </ul>"},{"location":"reports/week08_ch10_11/","title":"Report \u2013 Week 8, Chapters 10\u201311: CNNs &amp; Residual Neural Networks","text":"<p>Presentation Date: 15.12.2025\\ Presenter: Aisaiah Pellecer\\ Discussion Lead: Fadi Dalbah\\ Report by: Aisaiah Pellecer</p>"},{"location":"reports/week08_ch10_11/#chapter-10-convolutional-neural-networks","title":"Chapter 10: Convolutional Neural Networks","text":""},{"location":"reports/week08_ch10_11/#summary","title":"Summary","text":"<p>Convolutional Networks consist of convolutional layers, where each hidden unit is computed as the sum of nearby inputs, an added bias, and an activation function. Information is retained by repeating this operation with different weights and biases to create multiple channels at each spatial position.</p> <p>Most CNN models, such as encoder\u2013decoder models, use a sequence of layers that change the spatial size of the data as well as the number of channels (by upsampling or downsampling). Toward the end of the network, fully connected layers are often used to produce the final output.</p> <p>The structure of these models offers key performance benefits that overcome the limitations of fully connected networks.</p> <ol> <li> <p>The translational equivariance of convolutional layers is especially powerful for image-based tasks. For example, this means that the representation of an object in a given image remains consistent despite shifts in its position. In other words, the model encodes the inductive bias that can be applied to image classification, object detection, and semantic segmentation networks.</p> </li> <li> <p>Compared to fully connected networks, weights and biases are shared across every spatial position. Furthermore, there are far fewer parameters, and larger inputs (e.g., larger image sizes) can be handled.</p> </li> </ol>"},{"location":"reports/week08_ch10_11/#common-cnn-architectures","title":"Common CNN Architectures","text":"<ul> <li>Image Classification: AlexNet</li> <li>Object Detection: YOLO (You Only Look Once) Network</li> <li>Semantic Segmentation: Encoder\u2013Decoder Networks or Hourglass Networks</li> </ul>"},{"location":"reports/week08_ch10_11/#chapter-11-residual-neural-networks","title":"Chapter 11: Residual Neural Networks","text":"<p>Residual Neural Networks use residual blocks (or residual layers), which allow much deeper networks to be trained. The ability to train deeper networks is due to residual blocks, which compute an additive change to the current representation instead of transforming it directly.</p> <p>Residual connections allow the final output to be expressed as the sum of the original input and the outputs of multiple shorter networks. This enables information from the original input to be preserved as the network depth increases.</p> <p>Limitations of Sequential Processing</p> <p>In the previous chapter, it was noted that Convolutional Neural Networks have specific features that allow them to outperform fully connected networks. However, the input size of convolutional networks cannot be arbitrarily increased, as performance tends to degrade as the network becomes deeper. This limitation is a direct result of sequential processing network models, where inputs and outputs are passed from one hidden unit to the next in a strictly sequential manner.</p> <p>As networks grow deeper, small changes to parameters in early layers can cause large and unpredictable changes in the loss gradients. Even with proper weight initialization that avoids vanishing or exploding gradients, optimization algorithms rely on finite step sizes, which can move the model to regions of the loss surface with unrelated gradients. As a result, the loss surface can become highly irregular (known as shattered gradients), making optimization increasingly difficult in deep sequential networks.</p>"},{"location":"reports/week08_ch10_11/#exploding-gradients-and-batch-normalization","title":"Exploding Gradients and Batch Normalization","text":"<p>While residual blocks help enable deeper network training and mitigate the problem of vanishing gradients, they are not immune to exploding gradients. They can suffer from unstable forward propagation, as there may be an exponential increase in the variance of activations during training.</p> <p>These issues are addressed using Batch Normalization, which is applied independently to each hidden unit. Batch Normalization shifts and rescales each activation so that its mean and variance across the batch are learned during training. This makes the network invariant to rescaling of the weights and biases that contribute to each activation.</p> <p>Cost of Batch Normalization: Batch Normalization introduces two additional parameters (mean and variance) at each hidden unit, which increases the model size. However, Batch Normalization enables:</p> <ol> <li>Stable Forward Propagation: With unit variance, the variance at initialization increases linearly with each residual block layer.</li> <li>Higher Learning Rates: The loss surface and gradients change more smoothly and predictably, allowing the model to learn solutions that generalize well.</li> <li>Regularization: Batch Normalization introduces noise during training, which improves generalization.</li> </ol>"},{"location":"reports/week08_ch10_11/#common-residual-architectures","title":"Common Residual Architectures","text":"<ul> <li>ResNets</li> <li>DenseNets</li> <li>U-Nets and Hourglass Networks</li> </ul>"},{"location":"reports/week08_ch10_11/#discussion-notes","title":"Discussion Notes","text":"<p>What are the uses for 1D-CNNs?</p> <p>1D-CNNs can be used for timeseries or any sequential data especially when only immediate context is needed for interpretation. For data where larger context is needed nowadays transformer would be the choice.</p> <p>What is stride good for?</p> <p>While stride is used for downsampling between layers when set to &gt;2 it only loses data. Pooling would be a preferable way to do downsampling since it doesn't downsample \"mindlessly\".</p> <p>What is a 1x1 convolution used for?</p> <p>A 1x1 convolution is used for downsampling or upsampling the channels layer.</p> <p>What are other applications for CNNs other than images?</p> <p>Any spatial data where immediate context between close data points matters are sensible to be used for training CNNs.</p> <p>What is the interpretation of a skip connection in a ResNet?</p> <p>A skip connection can be interpreted as a learned parameter on which layers contribute well to the output. They are used to prevent the vanishing gradient problem.</p> <p>Can we identify good/bad layers by removing them permanently and see how it affects the results?</p> <p>While it should be possible to do so, the effort will probably not be worth it.</p>"},{"location":"reports/week08_ch10_11/#references","title":"References","text":"<ul> <li>Deep Learning using Rectified Linear Units (ReLU) [Link]</li> </ul>"},{"location":"reports/week09_ch18/","title":"Report \u2013 Week 09: Diffusion Models","text":"<p>Presenter: Alexander Semler</p> <p>Date: 05.01.2026</p>"},{"location":"reports/week09_ch18/#presentation-summary","title":"Presentation Summary","text":""},{"location":"reports/week09_ch18/#overview","title":"Overview","text":""},{"location":"reports/week09_ch18/#diffusion-models-introduction","title":"Diffusion Models: Introduction","text":"<p>Core Idea: Generate new data by learning to reverse a gradual noise-addition process.</p> <p>Encoder (Forward Process) - Maps input x through latent variables z\u2081, \u2026, z_T - Gradually mixes data with noise - Pre-specified (no learning needed)</p> <p>Decoder (Reverse Process) - Learned neural network - Removes noise at each stage - Passes data back through latent variables</p> <p>Sampling: To generate an image, sample noise z_T ~ q(z_T) and pass through decoder.</p>"},{"location":"reports/week09_ch18/#notation-and-key-variables","title":"Notation and Key Variables","text":"<p>Data and Latent Variables - x \u2014 Original data example (e.g., an image) - z_t \u2014 Latent variable at diffusion step t   - t = 1, 2, \u2026, T where T is the total number of steps   - All z_t have the same dimensionality as x</p> <p>Noise Parameters - \u03b5_t \u2014 Random noise drawn from standard normal N(0, I) - \u03b2_t \u2014 Noise schedule parameter at step t   - Controls how much noise is added at each step   - \u03b2_t \u2208 [0, 1] (typically small values, e.g., 0.0001 to 0.02)</p>"},{"location":"reports/week09_ch18/#encoder","title":"Encoder","text":""},{"location":"reports/week09_ch18/#forward-diffusion-process","title":"Forward Diffusion Process","text":"<p>Diffusion Step:</p> <pre><code>z\u2081 = \u221a(1-\u03b2\u2081) \u00b7 x + \u221a\u03b2\u2081 \u00b7 \u03b5\u2081\nz_t = \u221a(1-\u03b2_t) \u00b7 z_{t-1} + \u221a\u03b2_t \u00b7 \u03b5_t    \u2200 t \u2208 2,\u2026,T\n</code></pre> <p>(Equation 18.1)</p> <p>Diffusion Step (probabilistic):</p> <pre><code>q(z\u2081|x) = Norm[\u221a(1-\u03b2\u2081) \u00b7 x, \u03b2\u2081I]\nq(z_t|z_{t-1}) = Norm[\u221a(1-\u03b2_t) \u00b7 z_{t-1}, \u03b2_tI]\n</code></pre> <p>(Equation 18.2)</p> <p>Joint distribution of all latent variables:</p> <pre><code>q(z_{1\u2026T}|x) = q(z\u2081|x) \u220f_{t=2}^T q(z_t|z_{t-1})\n</code></pre> <p>(Equation 18.3)</p>"},{"location":"reports/week09_ch18/#diffusion-kernel-qz_tx","title":"Diffusion Kernel q(z_t|x)","text":"<p>Goal: Since computing z_t iteratively can be highly time consuming, we want to find a closed form expression.</p> <p>Approach: By substituting the equation for z_{t-1} into z_t repeatedly, we can derive a closed form expression that skips all intermediate variables.</p> <p>Closed Form Expression (Diffusion Kernel):</p> <pre><code>z_t = \u221a\u03b1_t \u00b7 x + \u221a(1-\u03b1_t) \u00b7 \u03b5,    \u03b1_t = \u220f_{s=1}^t (1-\u03b2_s)\n</code></pre> <p>(Equation 18.7)</p> <pre><code>q(z_t|x) = Norm[\u221a\u03b1_t \u00b7 x, (1-\u03b1_t)I]\n</code></pre> <p>(Equation 18.8)</p>"},{"location":"reports/week09_ch18/#marginal-distribution-qz_t","title":"Marginal Distribution q(z_t)","text":"<pre><code>q(z_t) = \u222b\u222b q(z_{1\u2026t}|x)Pr(x) dz_{1\u2026t-1} dx\n</code></pre> <p>(Equation 18.9)</p> <pre><code>q(z_t) = \u222b q(z_t|x)Pr(x) dx\n</code></pre> <p>(Equation 18.10)</p> <p>Note: The marginal distribution q(z_t) cannot be written in closed form because we don't know the original data distribution Pr(x).</p>"},{"location":"reports/week09_ch18/#conditional-diffusion-distribution-qz_t-1z_t-x","title":"Conditional Diffusion Distribution q(z_{t-1}|z_t, x)","text":"<pre><code>q(z_{t-1}|z_t, x) = q(z_t|z_{t-1}, x)q(z_{t-1}|x) / q(z_t|x)\n</code></pre> <p>(Equation 18.12)</p> <pre><code>q(z_{t-1}|z_t, x) = Norm[(\u221a\u03b1_{t-1}\u03b2_t)/(1-\u03b1_t) \u00b7 x + (\u221a(1-\u03b2_t)(1-\u03b1_{t-1}))/(1-\u03b1_t) \u00b7 z_t, (\u03b2_t(1-\u03b1_{t-1}))/(1-\u03b1_t)I]\n</code></pre> <p>(Equation 18.15)</p> <p>Why is this useful? This distribution is used to train the decoder - it tells us the true distribution over z_{t-1} when we know both z_t and the training example x, which we do during training.</p>"},{"location":"reports/week09_ch18/#decoder","title":"Decoder","text":"<p>The Challenge: The true reverse distributions q(z_{t-1}|z_t) are complex, multi-modal distributions that depend on the unknown data distribution Pr(x).</p> <p>Approach: We approximate the reverse process with learned normal distributions:</p> <pre><code>Pr(z_T) = Norm[0, I]\nPr(z_{t-1}|z_t, \u03c6_t) = Norm[f_t[z_t, \u03c6_t], \u03c3_t\u00b2I]\nPr(x|z\u2081, \u03c6\u2081) = Norm[f\u2081[z\u2081, \u03c6\u2081], \u03c3\u2081\u00b2I]\n</code></pre> <p>(Equation 18.16)</p> <p>where f_t[z_t, \u03c6_t] is a neural network predicting the mean.</p>"},{"location":"reports/week09_ch18/#training","title":"Training","text":""},{"location":"reports/week09_ch18/#training-setup","title":"Training Setup","text":"<p>Joint Distribution:</p> <pre><code>Pr(x, z_{1\u2026T}|\u03c6_{1\u2026T}) = Pr(x|z\u2081, \u03c6\u2081) \u220f_{t=2}^T Pr(z_{t-1}|z_t, \u03c6_t) \u00b7 Pr(z_T)\n</code></pre> <p>(Equation 18.17)</p> <p>Likelihood:</p> <pre><code>Pr(x|\u03c6_{1\u2026T}) = \u222b Pr(x, z_{1\u2026T}|\u03c6_{1\u2026T}) dz_{1\u2026T}\n</code></pre> <p>(Equation 18.18)</p> <p>Training Objective: Maximize the log-likelihood of the training data {x_i}:</p> <pre><code>\u03c6\u0302_{1\u2026T} = argmax_{\u03c6_{1\u2026T}} [\u2211_{i=1}^I log Pr(x_i|\u03c6_{1\u2026T})]\n</code></pre> <p>(Equation 18.19)</p>"},{"location":"reports/week09_ch18/#training-elbo","title":"Training: ELBO","text":"<p>The Problem: The marginalization in equation 18.18 is intractable! We cannot directly maximize the log-likelihood.</p> <p>Evidence Lower Bound (ELBO):</p> <pre><code>ELBO[\u03c6_{1\u2026T}] = \u222b q(z_{1\u2026T}|x) log[Pr(x, z_{1\u2026T}|\u03c6_{1\u2026T}) / q(z_{1\u2026T}|x)] dz_{1\u2026T}\n</code></pre> <p>(Equation 18.21)</p> <pre><code>ELBO[\u03c6_{1\u2026T}] = E_{q(z\u2081|x)}[log Pr(x|z\u2081, \u03c6\u2081)]\n                 - \u2211_{t=2}^T E_{q(z_t|x)}[D_KL(q(z_{t-1}|z_t, x) || Pr(z_{t-1}|z_t, \u03c6_t))]\n</code></pre> <p>(Equation 18.25)</p>"},{"location":"reports/week09_ch18/#reparameterization","title":"Reparameterization","text":""},{"location":"reports/week09_ch18/#reparameterization-of-loss-function","title":"Reparameterization of Loss Function","text":"<p>Motivation: Predicting noise \u03b5 instead of z_{t-1} yields better empirical performance.</p> <p>Approach: 1. Reparameterize target: Express x in terms of noise: x = (1/\u221a\u03b1_t)z_t - (\u221a(1-\u03b1_t)/\u221a\u03b1_t)\u03b5 2. Reparameterize network: Replace f_t[z_t, \u03c6_t] (predicts z_{t-1}) with g_t[z_t, \u03c6_t] (predicts \u03b5)</p> <p>Final Simplified Loss:</p> <pre><code>L[\u03c6_{1\u2026T}] = \u2211_{i=1}^I \u2211_{t=1}^T ||g_t[\u221a\u03b1_t \u00b7 x_i + \u221a(1-\u03b1_t) \u00b7 \u03b5_{it}, \u03c6_t] - \u03b5_{it}||\u00b2\n</code></pre> <p>(Equation 18.40)</p> <p>Train network to predict the noise that was added to create z_t from x.</p>"},{"location":"reports/week09_ch18/#implementation","title":"Implementation","text":""},{"location":"reports/week09_ch18/#application-to-images","title":"Application to Images","text":"<p>Network Architecture: U-Net - Image-to-image mapping: noisy image \u2192 predicted noise - Single U-Net shared across all time steps t = 1, \u2026, T - Time step t encoded as sinusoidal embedding (like positional encoding)</p>"},{"location":"reports/week09_ch18/#improving-generation-speed","title":"Improving Generation Speed","text":"<p>Key Insight: The loss function (Eq. 18.40) is valid for any forward process with diffusion kernel q(z_t|x) = Norm[\u221a\u03b1_t \u00b7 x, \u221a(1-\u03b1_t)I]</p> <p>Denoising Diffusion Implicit Models - No longer stochastic after first step - i.e. from step z\u2081 to z_t</p> <p>Accelerated Sampling Models - Forward process only on sub-sequence of steps - Skip steps during reverse process</p> <p>Result: Much more efficient sampling</p>"},{"location":"reports/week09_ch18/#conditional-generation","title":"Conditional Generation","text":"<p>Goal: Control generation using labels c (class, text, etc.)</p> <p>Classifier Guidance: Modify denoising update to make class c more likely:</p> <pre><code>z_{t-1} = \u1e91_{t-1} + \u03c3_t\u00b2 \u00b7 \u2202log Pr(c|z_t)/\u2202z_t + \u03c3_t \u00b7 \u03b5\n</code></pre> <p>(Equation 18.41)</p> <ul> <li>Requires separate classifier Pr(c|z_t) trained on latent variables</li> <li>Classifier shared across time steps (takes t as input)</li> </ul> <p>Classifier-Free Guidance - Incorporate class directly into main model: g_t[z_t, \u03c6_t, c] - Add embedding of c to U-Net layers (similar to time embedding) - Train on both conditional and unconditional objectives (by randomly dropping class information)</p>"},{"location":"reports/week09_ch18/#improving-generation-quality","title":"Improving Generation Quality","text":"<p>Key Techniques:</p> <p>1. Estimate Variances: - Learn \u03c3_t\u00b2 in addition to mean - Particularly helps when sampling with fewer steps</p> <p>2. Adaptive Noise Schedule: - Vary \u03b2_t at each step for better results</p> <p>3. Cascaded Diffusion Models: - First model: generate low-resolution image - Subsequent models: progressively increase resolution - Condition on lower-resolution image + class/text info</p>"},{"location":"reports/week09_ch18/#discussion-notes","title":"Discussion Notes","text":"<ul> <li> <p>Unlike VAEs, Diffusion uses fixed forward noise (Gaussian).</p> <ul> <li>Removes encoder optimization. network focuses solely on the reverse denoising mapping.</li> </ul> </li> <li> <p>Model does not \"remember\" the original image to undo noise.</p> <ul> <li>Learns a gradient field pointing from noise $\\to$ high-probability data manifold. Result is a realistic image, (but not the original).</li> </ul> </li> <li> <p>Bottlenecks:</p> <ul> <li>Training: Parallelizable (can sample arbitrary $t$ immediately).</li> <li>Inference: Sequential (Markov Chain). 1000+ steps $\\to$ high latency vs. GANs (1 pass).</li> </ul> </li> <li> <p>Network predicts added noise, not the clean image $x$.</p> <ul> <li>Acts as residual learning. more stable than predicting means/images directly!</li> </ul> </li> </ul>"},{"location":"reports/week10_ch19/","title":"Report \u2013 Week 10: (Chapter 19) Reinforcement Learning","text":"<p>Presenter: Marc Gl\u00e4ser</p> <p>Date: 29.01.2026 </p>"},{"location":"reports/week10_ch19/#summary","title":"Summary","text":""},{"location":"reports/week10_ch19/#overview","title":"Overview","text":"<p>Reinforcement Learning (RL): A framework in which an agent learns a policy $\\pi$ to map states to actions in an environment, aiming to maximize the cumulative reward over time.</p> <p>Key Challenges: - Temporal Credit Assignment: Determining which past action caused a current reward is hard. - Exploration vs. Exploitation: Balancing trying new actions (gathering data) vs. using known best actions (maximizing reward).</p>"},{"location":"reports/week10_ch19/#1-markov-decision-processes-mdps","title":"1. Markov Decision Processes (MDPs)","text":"<p>Mathematical formalization of the RL problem: Definitions: - State ($s \\in \\mathcal{S}$): Complete description of the world configuration. (Everything is a state) - Action ($a \\in \\mathcal{A}$): discrete or continuous choices available to the agent. (For example left, right, up, down) - Transition ($P(s'|s, a)$): Probability of moving to state $s'$ given action $a$ in state $s$. (Can be 1 but normally isn't) - Reward ($r(s, a)$): Immediate scalar feedback signal.  - Discount Factor ($\\gamma \\in [0, 1]$): Weights future rewards. Ensures convergence.</p> <p>Goal: Maximize the Expected Return ($G_t$):</p> <p>$$G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}$$</p>"},{"location":"reports/week10_ch19/#2-value-functions-bellman-equations","title":"2. Value Functions &amp; Bellman Equations","text":"<p>To solve the credit assignment problem, we estimate the long-term value of states:</p> <p>Value Definitions: - State-Value Function $v_{\\pi}(s)$: Expected return starting from $s$ following policy $\\pi$. - Action-Value Function $q_{\\pi}(s, a)$: Expected return taking action $a$ in $s$, then following $\\pi$.</p> <p>The Bellman Expectation Equation:</p> <p>Decomposes value into immediate reward + discounted value of successor state. </p> <p>$$v_{\\pi}(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a) [r + \\gamma v_{\\pi}(s')]$$</p>"},{"location":"reports/week10_ch19/#3-learning-paradigms","title":"3. Learning Paradigms","text":"<p>Different options for computing these values are:</p> Method Description Characteristics Dynamic Programming (DP) Solves Bellman equations iteratively using the true Transition model $P$. (which we normally dont have) Model-Based. Requires full knowledge of environment. Monte Carlo (MC) Estimates value by averaging returns $G_t$ from complete episodes. Model-Free. High variance, unbiased. Requires termination. Temporal Difference (TD) Updates estimates based on other estimates (bootstrapping) after a single step. Model-Free. Lower variance, biased. Update target: $r + \\gamma v(s')$."},{"location":"reports/week10_ch19/#4-value-based-methods","title":"4. Value-Based Methods","text":"<p>Strategy: Learn the optimal action-value function $q^*(s,a)$ and define policy as $\\pi(s) = \\text{argmax}_a q(s,a)$.</p> <p>Tabular Q-Learning (TD Control): Updates $q$-values toward the optimal future value (off-policy), regardless of the actual action taken next.</p> <p>$$q(s,a) \\leftarrow q(s,a) + \\alpha [r + \\gamma \\max_{a'} q(s', a') - q(s,a)]$$</p> <p>Deep Q-Learning (DQN):</p> <p>Approximates the table with a Neural Network $f[s, \\phi] \\approx q^(s,a)$. - Loss Function: $\\mathcal{L}(\\phi) = (y - q(s, a, \\phi))^2$ - Target ($y$):* $r + \\gamma \\max_{a'} q(s', a', \\phi^-)$</p> <p>Stability Modifications:</p> <ol> <li>Experience Replay: Buffer $\\mathcal{D}$ stores transitions $(s,a,r,s')$. Random sampling breaks temporal correlation.</li> <li> <p>Target Networks: Uses frozen parameters $\\phi^-$ to compute targets, preventing \"chasing your own tail.\"</p> </li> <li> <p>Double DQN (DDQN):</p> <ul> <li>Problem: Max operator in Q-learning systematically overestimates values.</li> <li>Solution: Decouple selection (online net $\\phi$) and evaluation (target net $\\phi^-$).</li> <li>Target: $y = r + \\gamma q(s', \\text{argmax}_{a'} q(s', a', \\phi), \\phi^-)$.</li> </ul> </li> </ol>"},{"location":"reports/week10_ch19/#5-policy-based-methods-policy-gradients","title":"5. Policy-Based Methods (Policy Gradients)","text":"<p>Strategy: Parameterize the policy $\\pi(a|s, \\theta)$ directly and optimize expected return $J(\\theta)$ via Gradient Ascent.</p> <p>The Policy Gradient Theorem:</p> <p>The gradient update is calculated by using the Formular:</p> <p>$$ \\theta \\leftarrow \\theta+\\alpha \\cdot \\frac{1}{I} \\sum_{i=1}^I \\sum_{t=1}^T \\frac{\\partial \\log \\left[\\pi\\left[a_{i t} \\mid \\mathrm{s}{i t}, \\theta\\right]\\right]}{\\partial \\theta} \\sum{k=t}^T r_{i, k+1} . $$</p> <p>REINFORCE (Monte Carlo PG):</p> <p>Uses the full actual return $G_t$ from the episode. - Issue: High variance because $G_t$ depends on long stochastic trajectories.</p> <p>Actor-Critic (TD PG): Reduces variance by replacing $G_t$ with a lower-variance estimate. - Actor ($\\theta$): Updates policy using the Advantage. - Critic ($\\phi$): Learns value function $v(s, \\phi)$ to compute the baseline. - Advantage Function: Quantifies how much better an action was than expected.     $$A(s_t, a_t) = \\underbrace{r_t + \\gamma v(s_{t+1}, \\phi)}{\\text{TD Target}} - \\underbrace{v(s_t, \\phi)}{\\text{Baseline}}$$</p>"},{"location":"reports/week10_ch19/#discussion-notes","title":"Discussion Notes","text":"<p>What is reward?</p> <ul> <li>A reward indicates how good or bad an action is after leaving a state. An example is collecting some coin in video games such as the Super mario games.</li> </ul> <p>How can penalties be integrated?</p> <ul> <li>Penalties can be included by assigning negative values to the rewards.</li> </ul> <p>What problems occur when the reward are noisy?</p> <ul> <li>For Q-Learning, when the policy selects the action with the maximum state-action value, this will often be overestimated due to noise. This leads to overreaction and can result in a chain reaction, which causes   even more overreactions.</li> </ul> <p>In which order the actor and critic is performed?</p> <ul> <li>In general, the actor are critic are executed in an alternating order. However, in the book it is not specified, how often it is performed.</li> </ul> <p>How does the baseline need to be understood?</p> <ul> <li>The baseline is used to reduce the variance, which is typically high in policy gradient methods.</li> </ul> <p>When the optimal state values are known, what is the relation to the optimal policy?</p> <ul> <li>The optimal policy can be inferred by choosing the next action with the highest state value. This is a greedy approach.</li> </ul>"},{"location":"reports/week10_ch19/#references","title":"References","text":"<ul> <li>Understand Deep Learning</li> </ul>"}]}